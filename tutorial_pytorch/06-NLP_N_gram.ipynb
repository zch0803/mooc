{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Process\n",
    "\n",
    "### N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # whether GPU is supportted\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2]) for i in range(len(test_sentence)-2)]\n",
    "vocb = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size * n_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, self.n_word)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(1, -1)\n",
    "        out = self.linear1(emb)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        log_prob = F.log_softmax(out, dim=1)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n",
      "Loss: 5.382554\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 5.322231\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 5.262753\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 5.203866\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 5.145464\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 5.087427\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 5.029656\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.972167\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.914948\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.857956\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.801033\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.743916\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.686612\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 4.629117\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 4.571394\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 4.513298\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 4.454772\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 4.395670\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 4.336003\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 4.275900\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 4.215404\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 4.154387\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 4.092783\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 4.030643\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 3.967951\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 3.904687\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 3.841022\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 3.776984\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 3.712543\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 3.647807\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 3.582811\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 3.517635\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 3.452129\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 3.386770\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 3.321119\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 3.255327\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 3.189537\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 3.123564\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 3.057646\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 2.991655\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 2.925795\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 2.859810\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 2.793933\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 2.727891\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 2.661994\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 2.596107\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 2.530388\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 2.464866\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 2.399571\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 2.334787\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 2.270138\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 2.205992\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 2.142205\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 2.079046\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 2.016389\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 1.954504\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 1.893379\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 1.833123\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 1.773700\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 1.715157\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 1.657643\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 1.601062\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 1.545545\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 1.491201\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 1.438043\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 1.386059\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 1.335337\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 1.286014\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 1.238037\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 1.191213\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 1.146033\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 1.102137\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 1.059658\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 1.018579\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 0.978966\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 0.940733\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 0.903934\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 0.868648\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 0.834653\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 0.802126\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 0.770960\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 0.741075\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 0.712574\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 0.685266\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 0.659304\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 0.634446\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 0.610787\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 0.588263\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 0.566800\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 0.546399\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 0.526983\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 0.508503\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 0.490942\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 0.474289\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 0.458381\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 0.443334\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 0.429013\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 0.415400\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 0.402470\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 0.390160\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(f'epoch: {epoch+1}')\n",
    "    print('*' * 10)\n",
    "    running_loss =0\n",
    "    for data in trigram:\n",
    "        word, label = data\n",
    "        word = torch.LongTensor([word_to_idx[i] for i in word]).to(device)\n",
    "        label = torch.LongTensor([word_to_idx[label]]).to(device)\n",
    "        # forward\n",
    "        out = ngrammodel(word)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Loss: {running_loss/len(word_to_idx):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real word is thy, predict word is thy\n"
     ]
    }
   ],
   "source": [
    "word, label = trigram[3]\n",
    "word = torch.LongTensor([word_to_idx[i] for i in word]).to(device)\n",
    "out = ngrammodel(word)\n",
    "_, predict_label = torch.max(out, 1)\n",
    "predict_word = idx_to_word[predict_label.item()]\n",
    "print(f'real word is {label}, predict word is {predict_word}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
