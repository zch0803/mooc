{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Basics\n",
    "\n",
    "载入基本模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 基本处理单元\n",
    "\n",
    "**Tensor** 类是 torch 中的基本数据类型。在 0.4.0 版本开始，**Variable** 类和 **Tensor** 类开始合并统一，且自 0.4.0 版本开始，允许生成 0 维 tensor 变量。\n",
    "\n",
    "我们可以调用该类的实例化函数，得到一个 5x4 的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4382e-37,  0.0000e+00,  5.7453e-44,  0.0000e+00],\n",
       "        [        nan,  6.4460e-44,  1.3733e-14,  6.4076e+07],\n",
       "        [ 2.0706e-19,  7.3909e+22,  2.4176e-12,  1.1625e+33],\n",
       "        [ 8.9605e-01,  1.1632e+33,  5.6003e-02,  7.0374e+22],\n",
       "        [ 5.7453e-44,  5.7453e-44,  4.4721e+21,  2.8776e+32]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量的创建\n",
    "\n",
    "目前，tensor 对象的创建支持多种方式，例如 `torch.tensor()` 可以直接从 _list, tuple, np.ndarray, scalar_ 等生成 tensor 变量。而 `torch.from_numpy()` 则可以从 np.ndarray 获得一个数据的**浅拷贝**。其余也有类似 `torch.ones()`, `torch.zeros()`, `torch.arange()`, `torch.linspace()`, `torch.eye()` 等数据生成方式。同时也包括一些随机生成方式，可以参照文档。下面的例子就是返回一个 5x4 的随机矩阵，随机初始化为 0-1 的均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5879,  0.4390,  0.4041,  0.4001],\n",
      "        [ 0.4700,  0.1505,  0.0929,  0.3115],\n",
      "        [ 0.4930,  0.9751,  0.7643,  0.9618],\n",
      "        [ 0.2723,  0.1353,  0.4302,  0.3362],\n",
      "        [ 0.8675,  0.0024,  0.0486,  0.8003]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取一个矩阵的 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意\n",
    "\n",
    "比较 tensor 的类型时不能再使用 `type()` 方法，应该使用 `isinstance()` 或者 `x.type()` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.DoubleTensor\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.DoubleTensor([1, 1, 1])\n",
    "print(type(x))\n",
    "print(x.type())\n",
    "print(isinstance(x, torch.DoubleTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor 的属性\n",
    "\n",
    "每一个 Tensor 对象都有三个属性：`torch.dtype`, `torch.device`, `torch.layout` 以及 `requires_grad`\n",
    "\n",
    "#### torch.dtype\n",
    "\n",
    "Pytorch 有八种不同的 Tensor 类型，如下表所示：\n",
    "\n",
    "| Data type | dtype | Tensor types|\n",
    "| ---------:|----:|-----------:|\n",
    "| 32-bit floating point | `torch.float32` or `torch.float` | `torch.*.FloatTensor` |\n",
    "| 64-bit floating point | `torch.float64` or `torch.double` | `torch.*.DoubleTensor` |\n",
    "| 16-bit floating point | `torch.float16` or `torch.half` | `torch.*.HalfTensor` |\n",
    "| 8-bit integer (unsigned) | `torch.uint8` | `torch.*.ByteTensor` |\n",
    "| 8-bit integer (signed) | `torch.int8` | `torch.*.CharTensor` |\n",
    "| 16-bit integer (signed) | `torch.int16` or `torch.short` | `torch.*.ShortTensor` |\n",
    "| 32-bit integer (signed) | `torch.int32` or `torch.int` | `torch.*.IntTensor` |\n",
    "| 64-bit integer (signed) | `torch.int64` or `torch.long` | `torch.*.LongTensor` |\n",
    "\n",
    "#### torch.device\n",
    "\n",
    "torch.device 属性决定了数据分配内存的位置，目前推荐以下代码来决定不确定是否使用 GPU 运算：\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = data.to(device)\n",
    "model = MyModule(...).to(device)\n",
    "```\n",
    "\n",
    "#### torch.layout\n",
    "\n",
    "决定存储方式，目前只支持 `torch.strided` 和 `torch.sparse_coo` 两种方式。\n",
    "\n",
    "### requires_grad\n",
    "\n",
    "是否累积梯度，默认为 False，且该属性为`或`运算方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与 numpy 中数据类型的互相转换\n",
    "\n",
    "* `torch.tensor()` 方法支持从 np.ndarray 中复制数据并且生成一个 Tensor 对象；\n",
    "* `torch.from_numpy()` 方法仅可从 np.ndarray 中生成一个 Tensor 对象，且两者在内存上共享; （文档如此解释，但是实验并不是这样）\n",
    "* `Tensor.numpy()` 方法可以将一个 Tensor 对象转变成 np.ndarray 变量，且两者在内存上共享。\n",
    "\n",
    "> **注意**：一个在计算图中的 Tensor 对象转化为 np.ndarray 变量，可用 `Tensor.detach()` 方法避免浅拷贝，`detach()` 方法会产生一个脱离于计算图的新 Tensor 对象，并且其 requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At first, a is [1. 1. 1.]\n",
      "At first, b is tensor([ 1.,  1.,  1.], dtype=torch.float64)\n",
      "At first, c is tensor([ 1.,  1.,  1.], dtype=torch.float64)\n",
      "Now, a is [-1.  1.  1.]\n",
      "And b is tensor([ 1.,  1.,  1.], dtype=torch.float64)\n",
      "And c is tensor([-1.,  1.,  1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# convert data type between numpy and torch.Tensor\n",
    "a = np.array([1., 1., 1.])\n",
    "b = torch.tensor(a)\n",
    "c = torch.from_numpy(a)\n",
    "print(f'At first, a is {a}')\n",
    "print(f'At first, b is {b}')\n",
    "print(f'At first, c is {c}')\n",
    "a[0] = -1\n",
    "print(f'Now, a is {a}')\n",
    "print(f'And b is {b}')\n",
    "print(f'And c is {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96112955 0.03798503 0.4426818  0.44538188]\n",
      " [0.61590683 0.75431883 0.9350943  0.6664036 ]\n",
      " [0.23648971 0.64038146 0.5608739  0.00237489]\n",
      " [0.5874882  0.44959515 0.4957475  0.98695034]\n",
      " [0.955158   0.91224027 0.02251083 0.92844963]]\n"
     ]
    }
   ],
   "source": [
    "# convert data type between numpy and torch.Tensor\n",
    "a = torch.rand(5, 4)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本运算与 numpy 类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 4)\n",
    "y = torch.rand(5, 4)\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7482,  1.1246,  1.4485,  1.7168],\n",
      "        [ 1.0103,  2.7181,  1.5097,  0.6454],\n",
      "        [ 1.4979,  1.0452,  1.4803,  2.7384],\n",
      "        [ 2.7554,  1.6346,  0.6852,  1.0243],\n",
      "        [ 0.9178,  0.0983,  0.8559,  2.3039]])\n"
     ]
    }
   ],
   "source": [
    "print(c * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0866,  0.1379,  0.3320,  0.1083],\n",
      "        [ 0.0821,  0.3130,  0.0868,  0.0392],\n",
      "        [ 0.0685,  0.2789,  0.1787,  0.0566],\n",
      "        [ 0.7293,  0.2979,  0.1071,  0.0980],\n",
      "        [ 0.2436,  0.0306,  0.2122,  0.3030]])\n"
     ]
    }
   ],
   "source": [
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7313,  0.7428,  1.1705,  0.7615],\n",
      "        [ 0.5807,  1.2514,  0.6758,  0.3974],\n",
      "        [ 0.6366,  1.1490,  0.8557,  0.9748],\n",
      "        [ 1.7126,  1.0916,  0.6974,  0.6285],\n",
      "        [ 1.1023,  0.9661,  1.0291,  1.1626]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7313,  0.7428,  1.1705,  0.7615],\n",
      "        [ 0.5807,  1.2514,  0.6758,  0.3974],\n",
      "        [ 0.6366,  1.1490,  0.8557,  0.9748],\n",
      "        [ 1.7126,  1.0916,  0.6974,  0.6285],\n",
      "        [ 1.1023,  0.9661,  1.0291,  1.1626]])\n"
     ]
    }
   ],
   "source": [
    "print(x.add(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.add_() 函数可以直接改变 x 的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7313,  0.7428,  1.1705,  0.7615],\n",
       "        [ 0.5807,  1.2514,  0.6758,  0.3974],\n",
       "        [ 0.6366,  1.1490,  0.8557,  0.9748],\n",
       "        [ 1.7126,  1.0916,  0.6974,  0.6285],\n",
       "        [ 1.1023,  0.9661,  1.0291,  1.1626]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7313,  0.7428,  1.1705,  0.7615],\n",
      "        [ 0.5807,  1.2514,  0.6758,  0.3974],\n",
      "        [ 0.6366,  1.1490,  0.8557,  0.9748],\n",
      "        [ 1.7126,  1.0916,  0.6974,  0.6285],\n",
      "        [ 1.1023,  0.9661,  1.0291,  1.1626]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将 torch.Tensor 放到 GPU 上（该方式已可以弃用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout whether your machine supports GPU calculation\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6496,  0.7672,  0.0563,  0.2528],\n",
      "        [ 0.5232,  0.6366,  0.0891,  0.7203],\n",
      "        [ 0.4789,  0.0279,  0.3992,  0.5644],\n",
      "        [ 0.8771,  0.7276,  0.3273,  0.5362],\n",
      "        [ 0.2573,  0.5473,  0.1579,  0.2247]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 4)\n",
    "if torch.cuda.is_available():\n",
    "    a = a.cuda()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余可用 cuda 功能的 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU device: 1\n",
      "tensor([[ 0.3854,  0.7269,  0.7160,  0.5512],\n",
      "        [ 0.6965,  0.1586,  0.1685,  0.1879]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "b = torch.rand(2, 4)\n",
    "if torch.cuda.is_available():\n",
    "    with torch.cuda.device(1):\n",
    "        print(f'Current GPU device: {torch.cuda.current_device()}')\n",
    "        b = b.cuda()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 nn.DataParallel 替代 multiprocessing\n",
    "\n",
    "大多数涉及批量输入和多个GPU的情况应默认使用`DataParallel`来使用多个GPU。尽管有GIL的存在，单个python进程也可能使多个GPU饱和。\n",
    "\n",
    "从0.1.9版本开始，大量的GPU(8+)可能未被充分利用。然而，这是一个已知的问题，也正在积极开发。和往常一样，测试你的用例吧。\n",
    "\n",
    "调用`multiprocessing`来利用CUDA模型存在重要的注意事项；使用具有多处理功能的CUDA模型有重要的注意事项; 除非就是需要谨慎地满足数据处理需求，否则您的程序很可能会出现错误或未定义的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 的自动求导功能\n",
    "\n",
    "torch 和大部分框架一样有着自动求导功能，\n",
    "\n",
    "> 在0.4.0版本之前，只可以用 torch.autograd.Variable 创建可求导变量，\n",
    "> 旧版本的 Variable 和 Tensor 本质上也没有什么区别，不过 Variable 会放在一个计算图里面，可以进行前向传播和反向传播以及求导\n",
    "> 而在 0.4.0 版本之后，Tensor 和 Variable 合并为一个类\n",
    "\n",
    "旧版本 Variable 示意图：![1.png](http://pytorch.org/tutorials/_images/Variable.png)\n",
    "\n",
    "我们可以通过调用 Variable 对象的 **.data** 属性计算得到原始的 Tensor 变量（现在合并后已不推荐使用），而变量的累积梯度可以通过 **.grad** 属性获得。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动求导中有一个重要的功能 function，即每个变量的 **.grad_fn** 属性，该属性记录了创造该变量的函数过程，因此用户起始创造的变量该属性为空。以下即为从 Tensor 中创造变量的示例，其中 requires_grad 表示是否计算梯度，默认值为 False\n",
    "\n",
    "> 旧版本中除 requires_grad，还有 volatite 决定是否求导，现已经抛弃后一参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x's .grad_fn is None: None\n",
      "Tensor z's has .grad_fn: <AddBackward0 object at 0x7f956131ae80>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y = torch.tensor([5.], requires_grad=True)\n",
    "print(f'Tensor x\\'s .grad_fn is None: {x.grad_fn}')\n",
    "z = 2 * x + y + 4\n",
    "print(f'Tensor z\\'s has .grad_fn: {z.grad_fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算生成 z 的累积梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终打印关于各变量的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor([ 2.])\n",
      "dz/dy: tensor([ 1.])\n"
     ]
    }
   ],
   "source": [
    "print(f'dz/dx: {x.grad}')\n",
    "print(f'dz/dy: {y.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络部分\n",
    "\n",
    "所依赖的主要是 torch.nn 和 torch.nn.functional\n",
    "\n",
    "torch.nn 里面有着所有的神经网络的层的操作，其用来构建网络，只有执行一次网络的运算才执行一次；\n",
    "\n",
    "torch.nn.functional 里面包含的接口函数更加复杂，可以实现更灵活的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络的类框架，以下为包含一个卷积层的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # add a convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(2, 1, 4)\n",
    "        # more information of various layers could be found in pytorch's manual\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward propagation\n",
    "        x = self.conv1(x)\n",
    "        out = self.conv2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化的网络对象可以直接打印其结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0928, -0.3174, -0.3287, -0.3080, -0.0933],\n",
      "          [-0.3482,  0.0390, -0.0953, -0.1080, -0.1015],\n",
      "          [-0.1584, -0.1685, -0.5402,  0.0240, -0.5067],\n",
      "          [-0.2684,  0.1960,  0.0430,  0.0274, -0.3709],\n",
      "          [-0.0219, -0.3011, -0.1487, -0.2501, -0.1467]]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 10, 10)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
