{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "2018-04-25：更新为新版本代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # whether GPU is supportted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入/下载训练集 MNIST 手写数字训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('../_data/mnist', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST('../_data/mnist', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 Logistic Regression 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(Logistic_Regression, self).__init__()\n",
    "        self.logistic = nn.Linear(in_dim, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.logistic(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logistic_Regression(28 * 28, 10).to(device) # pictures' size are 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂回归与线性回归的区别可以在 loss 函数的定义上，Logistic 回归的损失函数为交叉熵损失函数，即\n",
    "\n",
    "$$L_H(\\mathbf x,\\mathbf z)=-\\sum_{k=1}^dx_k\\log z_k+(1-x_k)\\log(1-z_k).$$\n",
    "\n",
    "优化器与前者一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "[1/100] Loss: 2.189628, Acc: 0.269896\n",
      "[1/100] Loss: 2.054564, Acc: 0.458542\n",
      "[1/100] Loss: 1.941132, Acc: 0.548090\n",
      "[1/100] Loss: 1.842568, Acc: 0.600078\n",
      "[1/100] Loss: 1.755603, Acc: 0.636354\n",
      "[1/100] Loss: 1.680480, Acc: 0.662031\n",
      "Finish 1 epoch, Loss: 1.663267, Acc: 0.667633\n",
      "Test Loss: 1.212778, Acc: 0.808900\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 2\n",
      "[2/100] Loss: 1.192363, Acc: 0.806250\n",
      "[2/100] Loss: 1.158313, Acc: 0.806927\n",
      "[2/100] Loss: 1.129286, Acc: 0.806736\n",
      "[2/100] Loss: 1.099719, Acc: 0.810391\n",
      "[2/100] Loss: 1.075341, Acc: 0.812583\n",
      "[2/100] Loss: 1.051600, Acc: 0.814635\n",
      "Finish 2 epoch, Loss: 1.045255, Acc: 0.815400\n",
      "Test Loss: 0.883197, Acc: 0.835800\n",
      "Time: 9.2\n",
      "**********\n",
      "epoch 3\n",
      "[3/100] Loss: 0.887939, Acc: 0.829479\n",
      "[3/100] Loss: 0.879559, Acc: 0.829948\n",
      "[3/100] Loss: 0.864300, Acc: 0.832396\n",
      "[3/100] Loss: 0.851407, Acc: 0.834141\n",
      "[3/100] Loss: 0.840726, Acc: 0.835562\n",
      "[3/100] Loss: 0.831778, Acc: 0.835330\n",
      "Finish 3 epoch, Loss: 0.828071, Acc: 0.836000\n",
      "Test Loss: 0.737549, Acc: 0.850500\n",
      "Time: 9.2\n",
      "**********\n",
      "epoch 4\n",
      "[4/100] Loss: 0.762866, Acc: 0.841875\n",
      "[4/100] Loss: 0.748477, Acc: 0.844063\n",
      "[4/100] Loss: 0.738453, Acc: 0.845208\n",
      "[4/100] Loss: 0.730970, Acc: 0.845729\n",
      "[4/100] Loss: 0.724874, Acc: 0.846104\n",
      "[4/100] Loss: 0.720605, Acc: 0.846458\n",
      "Finish 4 epoch, Loss: 0.718653, Acc: 0.847033\n",
      "Test Loss: 0.654687, Acc: 0.861500\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 5\n",
      "[5/100] Loss: 0.668920, Acc: 0.857500\n",
      "[5/100] Loss: 0.667100, Acc: 0.857031\n",
      "[5/100] Loss: 0.663734, Acc: 0.854618\n",
      "[5/100] Loss: 0.659907, Acc: 0.854714\n",
      "[5/100] Loss: 0.656265, Acc: 0.854604\n",
      "[5/100] Loss: 0.653135, Acc: 0.854774\n",
      "Finish 5 epoch, Loss: 0.651568, Acc: 0.855000\n",
      "Test Loss: 0.600414, Acc: 0.866800\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 6\n",
      "[6/100] Loss: 0.624898, Acc: 0.856667\n",
      "[6/100] Loss: 0.621276, Acc: 0.859010\n",
      "[6/100] Loss: 0.614784, Acc: 0.860382\n",
      "[6/100] Loss: 0.613995, Acc: 0.859505\n",
      "[6/100] Loss: 0.609602, Acc: 0.860104\n",
      "[6/100] Loss: 0.606224, Acc: 0.860642\n",
      "Finish 6 epoch, Loss: 0.605655, Acc: 0.860750\n",
      "Test Loss: 0.561932, Acc: 0.871800\n",
      "Time: 9.0\n",
      "**********\n",
      "epoch 7\n",
      "[7/100] Loss: 0.577155, Acc: 0.866771\n",
      "[7/100] Loss: 0.575997, Acc: 0.865521\n",
      "[7/100] Loss: 0.577553, Acc: 0.863993\n",
      "[7/100] Loss: 0.577733, Acc: 0.863151\n",
      "[7/100] Loss: 0.575047, Acc: 0.864083\n",
      "[7/100] Loss: 0.572570, Acc: 0.864670\n",
      "Finish 7 epoch, Loss: 0.571854, Acc: 0.864600\n",
      "Test Loss: 0.532824, Acc: 0.876000\n",
      "Time: 9.6\n",
      "**********\n",
      "epoch 8\n",
      "[8/100] Loss: 0.548976, Acc: 0.865729\n",
      "[8/100] Loss: 0.548839, Acc: 0.867031\n",
      "[8/100] Loss: 0.548185, Acc: 0.867431\n",
      "[8/100] Loss: 0.547144, Acc: 0.867656\n",
      "[8/100] Loss: 0.546567, Acc: 0.868250\n",
      "[8/100] Loss: 0.545840, Acc: 0.868212\n",
      "Finish 8 epoch, Loss: 0.545752, Acc: 0.868100\n",
      "Test Loss: 0.510087, Acc: 0.878800\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 9\n",
      "[9/100] Loss: 0.538796, Acc: 0.867500\n",
      "[9/100] Loss: 0.534385, Acc: 0.869479\n",
      "[9/100] Loss: 0.532904, Acc: 0.870764\n",
      "[9/100] Loss: 0.530553, Acc: 0.870026\n",
      "[9/100] Loss: 0.527337, Acc: 0.870979\n",
      "[9/100] Loss: 0.525434, Acc: 0.871406\n",
      "Finish 9 epoch, Loss: 0.524866, Acc: 0.871517\n",
      "Test Loss: 0.491573, Acc: 0.882300\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 10\n",
      "[10/100] Loss: 0.512314, Acc: 0.872917\n",
      "[10/100] Loss: 0.505638, Acc: 0.875938\n",
      "[10/100] Loss: 0.510021, Acc: 0.874028\n",
      "[10/100] Loss: 0.511814, Acc: 0.874010\n",
      "[10/100] Loss: 0.508623, Acc: 0.874750\n",
      "[10/100] Loss: 0.508395, Acc: 0.873941\n",
      "Finish 10 epoch, Loss: 0.507690, Acc: 0.874067\n",
      "Test Loss: 0.476192, Acc: 0.883900\n",
      "Time: 9.6\n",
      "**********\n",
      "epoch 11\n",
      "[11/100] Loss: 0.505453, Acc: 0.873646\n",
      "[11/100] Loss: 0.499606, Acc: 0.875208\n",
      "[11/100] Loss: 0.498462, Acc: 0.875000\n",
      "[11/100] Loss: 0.497254, Acc: 0.874662\n",
      "[11/100] Loss: 0.494142, Acc: 0.875125\n",
      "[11/100] Loss: 0.494069, Acc: 0.875677\n",
      "Finish 11 epoch, Loss: 0.493260, Acc: 0.876150\n",
      "Test Loss: 0.463025, Acc: 0.885300\n",
      "Time: 9.6\n",
      "**********\n",
      "epoch 12\n",
      "[12/100] Loss: 0.484941, Acc: 0.876563\n",
      "[12/100] Loss: 0.489253, Acc: 0.875104\n",
      "[12/100] Loss: 0.486180, Acc: 0.875729\n",
      "[12/100] Loss: 0.483858, Acc: 0.876068\n",
      "[12/100] Loss: 0.480619, Acc: 0.878208\n",
      "[12/100] Loss: 0.479775, Acc: 0.879063\n",
      "Finish 12 epoch, Loss: 0.480917, Acc: 0.878433\n",
      "Test Loss: 0.451845, Acc: 0.886200\n",
      "Time: 7.9\n",
      "**********\n",
      "epoch 13\n",
      "[13/100] Loss: 0.468556, Acc: 0.877500\n",
      "[13/100] Loss: 0.469939, Acc: 0.878958\n",
      "[13/100] Loss: 0.472776, Acc: 0.877813\n",
      "[13/100] Loss: 0.474589, Acc: 0.878021\n",
      "[13/100] Loss: 0.471191, Acc: 0.879708\n",
      "[13/100] Loss: 0.471638, Acc: 0.879566\n",
      "Finish 13 epoch, Loss: 0.470207, Acc: 0.880033\n",
      "Test Loss: 0.442110, Acc: 0.888000\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 14\n",
      "[14/100] Loss: 0.461494, Acc: 0.881250\n",
      "[14/100] Loss: 0.460519, Acc: 0.882500\n",
      "[14/100] Loss: 0.464082, Acc: 0.881146\n",
      "[14/100] Loss: 0.463308, Acc: 0.881302\n",
      "[14/100] Loss: 0.464298, Acc: 0.880500\n",
      "[14/100] Loss: 0.461446, Acc: 0.881510\n",
      "Finish 14 epoch, Loss: 0.460802, Acc: 0.881600\n",
      "Test Loss: 0.433565, Acc: 0.889100\n",
      "Time: 9.4\n",
      "**********\n",
      "epoch 15\n",
      "[15/100] Loss: 0.448679, Acc: 0.885104\n",
      "[15/100] Loss: 0.449792, Acc: 0.884792\n",
      "[15/100] Loss: 0.454153, Acc: 0.882083\n",
      "[15/100] Loss: 0.451128, Acc: 0.883672\n",
      "[15/100] Loss: 0.452430, Acc: 0.883646\n",
      "[15/100] Loss: 0.452476, Acc: 0.882795\n",
      "Finish 15 epoch, Loss: 0.452462, Acc: 0.882717\n",
      "Test Loss: 0.425957, Acc: 0.890700\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 16\n",
      "[16/100] Loss: 0.451403, Acc: 0.881250\n",
      "[16/100] Loss: 0.453716, Acc: 0.882969\n",
      "[16/100] Loss: 0.448993, Acc: 0.884167\n",
      "[16/100] Loss: 0.448099, Acc: 0.883516\n",
      "[16/100] Loss: 0.447199, Acc: 0.884333\n",
      "[16/100] Loss: 0.446303, Acc: 0.883924\n",
      "Finish 16 epoch, Loss: 0.445008, Acc: 0.884367\n",
      "Test Loss: 0.419031, Acc: 0.891700\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 17\n",
      "[17/100] Loss: 0.445433, Acc: 0.883229\n",
      "[17/100] Loss: 0.442942, Acc: 0.884583\n",
      "[17/100] Loss: 0.441602, Acc: 0.885069\n",
      "[17/100] Loss: 0.442704, Acc: 0.883516\n",
      "[17/100] Loss: 0.439995, Acc: 0.884708\n",
      "[17/100] Loss: 0.439381, Acc: 0.885191\n",
      "Finish 17 epoch, Loss: 0.438278, Acc: 0.885683\n",
      "Test Loss: 0.412858, Acc: 0.893000\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 18\n",
      "[18/100] Loss: 0.430488, Acc: 0.885000\n",
      "[18/100] Loss: 0.427762, Acc: 0.888177\n",
      "[18/100] Loss: 0.427779, Acc: 0.887917\n",
      "[18/100] Loss: 0.432833, Acc: 0.886302\n",
      "[18/100] Loss: 0.434278, Acc: 0.885979\n",
      "[18/100] Loss: 0.433196, Acc: 0.886354\n",
      "Finish 18 epoch, Loss: 0.432162, Acc: 0.886750\n",
      "Test Loss: 0.407420, Acc: 0.894500\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 19\n",
      "[19/100] Loss: 0.432281, Acc: 0.884688\n",
      "[19/100] Loss: 0.425503, Acc: 0.889063\n",
      "[19/100] Loss: 0.423847, Acc: 0.888611\n",
      "[19/100] Loss: 0.425784, Acc: 0.887969\n",
      "[19/100] Loss: 0.425857, Acc: 0.887771\n",
      "[19/100] Loss: 0.426714, Acc: 0.887622\n",
      "Finish 19 epoch, Loss: 0.426589, Acc: 0.887717\n",
      "Test Loss: 0.402159, Acc: 0.894900\n",
      "Time: 8.5\n",
      "**********\n",
      "epoch 20\n",
      "[20/100] Loss: 0.417231, Acc: 0.887292\n",
      "[20/100] Loss: 0.425354, Acc: 0.886875\n",
      "[20/100] Loss: 0.422083, Acc: 0.888264\n",
      "[20/100] Loss: 0.421853, Acc: 0.888646\n",
      "[20/100] Loss: 0.423866, Acc: 0.887854\n",
      "[20/100] Loss: 0.421885, Acc: 0.888507\n",
      "Finish 20 epoch, Loss: 0.421478, Acc: 0.888633\n",
      "Test Loss: 0.397526, Acc: 0.896400\n",
      "Time: 9.0\n",
      "**********\n",
      "epoch 21\n",
      "[21/100] Loss: 0.416992, Acc: 0.890729\n",
      "[21/100] Loss: 0.423818, Acc: 0.886927\n",
      "[21/100] Loss: 0.421381, Acc: 0.888438\n",
      "[21/100] Loss: 0.419071, Acc: 0.889505\n",
      "[21/100] Loss: 0.419948, Acc: 0.889042\n",
      "[21/100] Loss: 0.417230, Acc: 0.889514\n",
      "Finish 21 epoch, Loss: 0.416761, Acc: 0.889583\n",
      "Test Loss: 0.393134, Acc: 0.897000\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 22\n",
      "[22/100] Loss: 0.425308, Acc: 0.887188\n",
      "[22/100] Loss: 0.419202, Acc: 0.888906\n",
      "[22/100] Loss: 0.415005, Acc: 0.889688\n",
      "[22/100] Loss: 0.410888, Acc: 0.890807\n",
      "[22/100] Loss: 0.410304, Acc: 0.891250\n",
      "[22/100] Loss: 0.412855, Acc: 0.890417\n",
      "Finish 22 epoch, Loss: 0.412390, Acc: 0.890317\n",
      "Test Loss: 0.389118, Acc: 0.897900\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 23\n",
      "[23/100] Loss: 0.395012, Acc: 0.895417\n",
      "[23/100] Loss: 0.406686, Acc: 0.891979\n",
      "[23/100] Loss: 0.407853, Acc: 0.891910\n",
      "[23/100] Loss: 0.410598, Acc: 0.891276\n",
      "[23/100] Loss: 0.409266, Acc: 0.891375\n",
      "[23/100] Loss: 0.408459, Acc: 0.891215\n",
      "Finish 23 epoch, Loss: 0.408334, Acc: 0.891233\n",
      "Test Loss: 0.385346, Acc: 0.898700\n",
      "Time: 9.6\n",
      "**********\n",
      "epoch 24\n",
      "[24/100] Loss: 0.400521, Acc: 0.888333\n",
      "[24/100] Loss: 0.401633, Acc: 0.889792\n",
      "[24/100] Loss: 0.405782, Acc: 0.890000\n",
      "[24/100] Loss: 0.405105, Acc: 0.891458\n",
      "[24/100] Loss: 0.406487, Acc: 0.891062\n",
      "[24/100] Loss: 0.404439, Acc: 0.891771\n",
      "Finish 24 epoch, Loss: 0.404544, Acc: 0.891783\n",
      "Test Loss: 0.381875, Acc: 0.899400\n",
      "Time: 9.0\n",
      "**********\n",
      "epoch 25\n",
      "[25/100] Loss: 0.403487, Acc: 0.890312\n",
      "[25/100] Loss: 0.398153, Acc: 0.893229\n",
      "[25/100] Loss: 0.401220, Acc: 0.891563\n",
      "[25/100] Loss: 0.400151, Acc: 0.891641\n",
      "[25/100] Loss: 0.399514, Acc: 0.892354\n",
      "[25/100] Loss: 0.400661, Acc: 0.892778\n",
      "Finish 25 epoch, Loss: 0.401013, Acc: 0.892650\n",
      "Test Loss: 0.378721, Acc: 0.900400\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 26\n",
      "[26/100] Loss: 0.392845, Acc: 0.894271\n",
      "[26/100] Loss: 0.397088, Acc: 0.892969\n",
      "[26/100] Loss: 0.395872, Acc: 0.893472\n",
      "[26/100] Loss: 0.396782, Acc: 0.894089\n",
      "[26/100] Loss: 0.397293, Acc: 0.893333\n",
      "[26/100] Loss: 0.397546, Acc: 0.893281\n",
      "Finish 26 epoch, Loss: 0.397696, Acc: 0.893117\n",
      "Test Loss: 0.375659, Acc: 0.900600\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 27\n",
      "[27/100] Loss: 0.391759, Acc: 0.894167\n",
      "[27/100] Loss: 0.389336, Acc: 0.894844\n",
      "[27/100] Loss: 0.388992, Acc: 0.895486\n",
      "[27/100] Loss: 0.390024, Acc: 0.894531\n",
      "[27/100] Loss: 0.390978, Acc: 0.894542\n",
      "[27/100] Loss: 0.393944, Acc: 0.893785\n",
      "Finish 27 epoch, Loss: 0.394558, Acc: 0.893567\n",
      "Test Loss: 0.372895, Acc: 0.900900\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 28\n",
      "[28/100] Loss: 0.392475, Acc: 0.893646\n",
      "[28/100] Loss: 0.387655, Acc: 0.895104\n",
      "[28/100] Loss: 0.391025, Acc: 0.895069\n",
      "[28/100] Loss: 0.394286, Acc: 0.893854\n",
      "[28/100] Loss: 0.394569, Acc: 0.893604\n",
      "[28/100] Loss: 0.391953, Acc: 0.894358\n",
      "Finish 28 epoch, Loss: 0.391615, Acc: 0.894367\n",
      "Test Loss: 0.370079, Acc: 0.901800\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 29\n",
      "[29/100] Loss: 0.386123, Acc: 0.894063\n",
      "[29/100] Loss: 0.383299, Acc: 0.894792\n",
      "[29/100] Loss: 0.386832, Acc: 0.894826\n",
      "[29/100] Loss: 0.386192, Acc: 0.895104\n",
      "[29/100] Loss: 0.387633, Acc: 0.895312\n",
      "[29/100] Loss: 0.389200, Acc: 0.894757\n",
      "Finish 29 epoch, Loss: 0.388824, Acc: 0.894783\n",
      "Test Loss: 0.367469, Acc: 0.902300\n",
      "Time: 9.2\n",
      "**********\n",
      "epoch 30\n",
      "[30/100] Loss: 0.380289, Acc: 0.896042\n",
      "[30/100] Loss: 0.389342, Acc: 0.893854\n",
      "[30/100] Loss: 0.387375, Acc: 0.894826\n",
      "[30/100] Loss: 0.383753, Acc: 0.896615\n",
      "[30/100] Loss: 0.384578, Acc: 0.895854\n",
      "[30/100] Loss: 0.385004, Acc: 0.895938\n",
      "Finish 30 epoch, Loss: 0.386188, Acc: 0.895700\n",
      "Test Loss: 0.365146, Acc: 0.902600\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 31\n",
      "[31/100] Loss: 0.385141, Acc: 0.895208\n",
      "[31/100] Loss: 0.388656, Acc: 0.893646\n",
      "[31/100] Loss: 0.387121, Acc: 0.895035\n",
      "[31/100] Loss: 0.384424, Acc: 0.895651\n",
      "[31/100] Loss: 0.382908, Acc: 0.895813\n",
      "[31/100] Loss: 0.383218, Acc: 0.896059\n",
      "Finish 31 epoch, Loss: 0.383683, Acc: 0.896050\n",
      "Test Loss: 0.362884, Acc: 0.903100\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 32\n",
      "[32/100] Loss: 0.383393, Acc: 0.893333\n",
      "[32/100] Loss: 0.384300, Acc: 0.894271\n",
      "[32/100] Loss: 0.380437, Acc: 0.896354\n",
      "[32/100] Loss: 0.382720, Acc: 0.895677\n",
      "[32/100] Loss: 0.382002, Acc: 0.896333\n",
      "[32/100] Loss: 0.381552, Acc: 0.896597\n",
      "Finish 32 epoch, Loss: 0.381305, Acc: 0.896617\n",
      "Test Loss: 0.360772, Acc: 0.903900\n",
      "Time: 9.0\n",
      "**********\n",
      "epoch 33\n",
      "[33/100] Loss: 0.373598, Acc: 0.897813\n",
      "[33/100] Loss: 0.378596, Acc: 0.897031\n",
      "[33/100] Loss: 0.380199, Acc: 0.895938\n",
      "[33/100] Loss: 0.379070, Acc: 0.897083\n",
      "[33/100] Loss: 0.379437, Acc: 0.897354\n",
      "[33/100] Loss: 0.378720, Acc: 0.897326\n",
      "Finish 33 epoch, Loss: 0.379013, Acc: 0.897200\n",
      "Test Loss: 0.358777, Acc: 0.904200\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 34\n",
      "[34/100] Loss: 0.376082, Acc: 0.896146\n",
      "[34/100] Loss: 0.374044, Acc: 0.898438\n",
      "[34/100] Loss: 0.372760, Acc: 0.898993\n",
      "[34/100] Loss: 0.379292, Acc: 0.896875\n",
      "[34/100] Loss: 0.375701, Acc: 0.897646\n",
      "[34/100] Loss: 0.376730, Acc: 0.897344\n",
      "Finish 34 epoch, Loss: 0.376846, Acc: 0.897450\n",
      "Test Loss: 0.356694, Acc: 0.904600\n",
      "Time: 8.9\n",
      "**********\n",
      "epoch 35\n",
      "[35/100] Loss: 0.365078, Acc: 0.902813\n",
      "[35/100] Loss: 0.369506, Acc: 0.900260\n",
      "[35/100] Loss: 0.375890, Acc: 0.897917\n",
      "[35/100] Loss: 0.373251, Acc: 0.898464\n",
      "[35/100] Loss: 0.373315, Acc: 0.898104\n",
      "[35/100] Loss: 0.375225, Acc: 0.897917\n",
      "Finish 35 epoch, Loss: 0.374800, Acc: 0.898183\n",
      "Test Loss: 0.354832, Acc: 0.905300\n",
      "Time: 6.8\n",
      "**********\n",
      "epoch 36\n",
      "[36/100] Loss: 0.372822, Acc: 0.895521\n",
      "[36/100] Loss: 0.367572, Acc: 0.898958\n",
      "[36/100] Loss: 0.371821, Acc: 0.898090\n",
      "[36/100] Loss: 0.372790, Acc: 0.898203\n",
      "[36/100] Loss: 0.373402, Acc: 0.898146\n",
      "[36/100] Loss: 0.373852, Acc: 0.898194\n",
      "Finish 36 epoch, Loss: 0.372807, Acc: 0.898483\n",
      "Test Loss: 0.353087, Acc: 0.905400\n",
      "Time: 9.2\n",
      "**********\n",
      "epoch 37\n",
      "[37/100] Loss: 0.349912, Acc: 0.906250\n",
      "[37/100] Loss: 0.360444, Acc: 0.903177\n",
      "[37/100] Loss: 0.364914, Acc: 0.901563\n",
      "[37/100] Loss: 0.370716, Acc: 0.899219\n",
      "[37/100] Loss: 0.369930, Acc: 0.899167\n",
      "[37/100] Loss: 0.371575, Acc: 0.898455\n",
      "Finish 37 epoch, Loss: 0.370927, Acc: 0.898850\n",
      "Test Loss: 0.351337, Acc: 0.906400\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 38\n",
      "[38/100] Loss: 0.367088, Acc: 0.898646\n",
      "[38/100] Loss: 0.372042, Acc: 0.897917\n",
      "[38/100] Loss: 0.369515, Acc: 0.899063\n",
      "[38/100] Loss: 0.366547, Acc: 0.900417\n",
      "[38/100] Loss: 0.367148, Acc: 0.900333\n",
      "[38/100] Loss: 0.369900, Acc: 0.899375\n",
      "Finish 38 epoch, Loss: 0.369098, Acc: 0.899517\n",
      "Test Loss: 0.349745, Acc: 0.906500\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 39\n",
      "[39/100] Loss: 0.365252, Acc: 0.899688\n",
      "[39/100] Loss: 0.367984, Acc: 0.898490\n",
      "[39/100] Loss: 0.369713, Acc: 0.898507\n",
      "[39/100] Loss: 0.367890, Acc: 0.899401\n",
      "[39/100] Loss: 0.370783, Acc: 0.898438\n",
      "[39/100] Loss: 0.368075, Acc: 0.899635\n",
      "Finish 39 epoch, Loss: 0.367363, Acc: 0.899817\n",
      "Test Loss: 0.348192, Acc: 0.907000\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 40\n",
      "[40/100] Loss: 0.350065, Acc: 0.904271\n",
      "[40/100] Loss: 0.364578, Acc: 0.900208\n",
      "[40/100] Loss: 0.365342, Acc: 0.900035\n",
      "[40/100] Loss: 0.365906, Acc: 0.899167\n",
      "[40/100] Loss: 0.365965, Acc: 0.900187\n",
      "[40/100] Loss: 0.365136, Acc: 0.900000\n",
      "Finish 40 epoch, Loss: 0.365677, Acc: 0.899983\n",
      "Test Loss: 0.346650, Acc: 0.907600\n",
      "Time: 9.7\n",
      "**********\n",
      "epoch 41\n",
      "[41/100] Loss: 0.363358, Acc: 0.901667\n",
      "[41/100] Loss: 0.362909, Acc: 0.901875\n",
      "[41/100] Loss: 0.364771, Acc: 0.901146\n",
      "[41/100] Loss: 0.363247, Acc: 0.900885\n",
      "[41/100] Loss: 0.363588, Acc: 0.900833\n",
      "[41/100] Loss: 0.364423, Acc: 0.900434\n",
      "Finish 41 epoch, Loss: 0.364066, Acc: 0.900400\n",
      "Test Loss: 0.345145, Acc: 0.907800\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 42\n",
      "[42/100] Loss: 0.364872, Acc: 0.902083\n",
      "[42/100] Loss: 0.362426, Acc: 0.902344\n",
      "[42/100] Loss: 0.362705, Acc: 0.900417\n",
      "[42/100] Loss: 0.362981, Acc: 0.900391\n",
      "[42/100] Loss: 0.362086, Acc: 0.900458\n",
      "[42/100] Loss: 0.362646, Acc: 0.900764\n",
      "Finish 42 epoch, Loss: 0.362511, Acc: 0.900717\n",
      "Test Loss: 0.343793, Acc: 0.908100\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 43\n",
      "[43/100] Loss: 0.358934, Acc: 0.901250\n",
      "[43/100] Loss: 0.358026, Acc: 0.902135\n",
      "[43/100] Loss: 0.358976, Acc: 0.901042\n",
      "[43/100] Loss: 0.358255, Acc: 0.901016\n",
      "[43/100] Loss: 0.360844, Acc: 0.900708\n",
      "[43/100] Loss: 0.361240, Acc: 0.900868\n",
      "Finish 43 epoch, Loss: 0.360996, Acc: 0.900950\n",
      "Test Loss: 0.342501, Acc: 0.908600\n",
      "Time: 9.6\n",
      "**********\n",
      "epoch 44\n",
      "[44/100] Loss: 0.351521, Acc: 0.903125\n",
      "[44/100] Loss: 0.355481, Acc: 0.902500\n",
      "[44/100] Loss: 0.359732, Acc: 0.900590\n",
      "[44/100] Loss: 0.360267, Acc: 0.900156\n",
      "[44/100] Loss: 0.362023, Acc: 0.900604\n",
      "[44/100] Loss: 0.359668, Acc: 0.901493\n",
      "Finish 44 epoch, Loss: 0.359558, Acc: 0.901583\n",
      "Test Loss: 0.341138, Acc: 0.908500\n",
      "Time: 8.1\n",
      "**********\n",
      "epoch 45\n",
      "[45/100] Loss: 0.360753, Acc: 0.900000\n",
      "[45/100] Loss: 0.358482, Acc: 0.901823\n",
      "[45/100] Loss: 0.356520, Acc: 0.902535\n",
      "[45/100] Loss: 0.353530, Acc: 0.903021\n",
      "[45/100] Loss: 0.356645, Acc: 0.902062\n",
      "[45/100] Loss: 0.356927, Acc: 0.902031\n",
      "Finish 45 epoch, Loss: 0.358155, Acc: 0.901800\n",
      "Test Loss: 0.339822, Acc: 0.907900\n",
      "Time: 9.4\n",
      "**********\n",
      "epoch 46\n",
      "[46/100] Loss: 0.349292, Acc: 0.905000\n",
      "[46/100] Loss: 0.354294, Acc: 0.902344\n",
      "[46/100] Loss: 0.354940, Acc: 0.902153\n",
      "[46/100] Loss: 0.357916, Acc: 0.900078\n",
      "[46/100] Loss: 0.356616, Acc: 0.901312\n",
      "[46/100] Loss: 0.355313, Acc: 0.902743\n",
      "Finish 46 epoch, Loss: 0.356804, Acc: 0.902217\n",
      "Test Loss: 0.338694, Acc: 0.909000\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 47\n",
      "[47/100] Loss: 0.360214, Acc: 0.896250\n",
      "[47/100] Loss: 0.359830, Acc: 0.898646\n",
      "[47/100] Loss: 0.355809, Acc: 0.900937\n",
      "[47/100] Loss: 0.354580, Acc: 0.901510\n",
      "[47/100] Loss: 0.353913, Acc: 0.902292\n",
      "[47/100] Loss: 0.354517, Acc: 0.902830\n",
      "Finish 47 epoch, Loss: 0.355503, Acc: 0.902483\n",
      "Test Loss: 0.337516, Acc: 0.909000\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 48\n",
      "[48/100] Loss: 0.354235, Acc: 0.904063\n",
      "[48/100] Loss: 0.353660, Acc: 0.903281\n",
      "[48/100] Loss: 0.349462, Acc: 0.904375\n",
      "[48/100] Loss: 0.351571, Acc: 0.903594\n",
      "[48/100] Loss: 0.352671, Acc: 0.903229\n",
      "[48/100] Loss: 0.354950, Acc: 0.902483\n",
      "Finish 48 epoch, Loss: 0.354233, Acc: 0.902683\n",
      "Test Loss: 0.336410, Acc: 0.909200\n",
      "Time: 9.6\n",
      "**********\n",
      "epoch 49\n",
      "[49/100] Loss: 0.356100, Acc: 0.902813\n",
      "[49/100] Loss: 0.351925, Acc: 0.904375\n",
      "[49/100] Loss: 0.349530, Acc: 0.904549\n",
      "[49/100] Loss: 0.350523, Acc: 0.904349\n",
      "[49/100] Loss: 0.351239, Acc: 0.904083\n",
      "[49/100] Loss: 0.352563, Acc: 0.903229\n",
      "Finish 49 epoch, Loss: 0.353008, Acc: 0.903117\n",
      "Test Loss: 0.335320, Acc: 0.909500\n",
      "Time: 9.4\n",
      "**********\n",
      "epoch 50\n",
      "[50/100] Loss: 0.352080, Acc: 0.903021\n",
      "[50/100] Loss: 0.343641, Acc: 0.904375\n",
      "[50/100] Loss: 0.346988, Acc: 0.903958\n",
      "[50/100] Loss: 0.346792, Acc: 0.904245\n",
      "[50/100] Loss: 0.351712, Acc: 0.903208\n",
      "[50/100] Loss: 0.351785, Acc: 0.903229\n",
      "Finish 50 epoch, Loss: 0.351824, Acc: 0.903183\n",
      "Test Loss: 0.334282, Acc: 0.909100\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 51\n",
      "[51/100] Loss: 0.352357, Acc: 0.901875\n",
      "[51/100] Loss: 0.351068, Acc: 0.902813\n",
      "[51/100] Loss: 0.354743, Acc: 0.902431\n",
      "[51/100] Loss: 0.353990, Acc: 0.903047\n",
      "[51/100] Loss: 0.354900, Acc: 0.902042\n",
      "[51/100] Loss: 0.352000, Acc: 0.903090\n",
      "Finish 51 epoch, Loss: 0.350662, Acc: 0.903283\n",
      "Test Loss: 0.333243, Acc: 0.909400\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 52\n",
      "[52/100] Loss: 0.354215, Acc: 0.903437\n",
      "[52/100] Loss: 0.347092, Acc: 0.905729\n",
      "[52/100] Loss: 0.347680, Acc: 0.904861\n",
      "[52/100] Loss: 0.349155, Acc: 0.904583\n",
      "[52/100] Loss: 0.350123, Acc: 0.903917\n",
      "[52/100] Loss: 0.348880, Acc: 0.903941\n",
      "Finish 52 epoch, Loss: 0.349537, Acc: 0.903767\n",
      "Test Loss: 0.332202, Acc: 0.909200\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 53\n",
      "[53/100] Loss: 0.348343, Acc: 0.902188\n",
      "[53/100] Loss: 0.356786, Acc: 0.900885\n",
      "[53/100] Loss: 0.348852, Acc: 0.903576\n",
      "[53/100] Loss: 0.347111, Acc: 0.904505\n",
      "[53/100] Loss: 0.350886, Acc: 0.903500\n",
      "[53/100] Loss: 0.348836, Acc: 0.904201\n",
      "Finish 53 epoch, Loss: 0.348457, Acc: 0.904183\n",
      "Test Loss: 0.331218, Acc: 0.910000\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 54\n",
      "[54/100] Loss: 0.339453, Acc: 0.910000\n",
      "[54/100] Loss: 0.344342, Acc: 0.906667\n",
      "[54/100] Loss: 0.345768, Acc: 0.906493\n",
      "[54/100] Loss: 0.340506, Acc: 0.907161\n",
      "[54/100] Loss: 0.343999, Acc: 0.905583\n",
      "[54/100] Loss: 0.346822, Acc: 0.904444\n",
      "Finish 54 epoch, Loss: 0.347398, Acc: 0.904317\n",
      "Test Loss: 0.330334, Acc: 0.909700\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 55\n",
      "[55/100] Loss: 0.333250, Acc: 0.911354\n",
      "[55/100] Loss: 0.337353, Acc: 0.908854\n",
      "[55/100] Loss: 0.345068, Acc: 0.905972\n",
      "[55/100] Loss: 0.347759, Acc: 0.904792\n",
      "[55/100] Loss: 0.348216, Acc: 0.904396\n",
      "[55/100] Loss: 0.346212, Acc: 0.904236\n",
      "Finish 55 epoch, Loss: 0.346365, Acc: 0.904283\n",
      "Test Loss: 0.329431, Acc: 0.909900\n",
      "Time: 9.8\n",
      "**********\n",
      "epoch 56\n",
      "[56/100] Loss: 0.347582, Acc: 0.904063\n",
      "[56/100] Loss: 0.344658, Acc: 0.905104\n",
      "[56/100] Loss: 0.343600, Acc: 0.905729\n",
      "[56/100] Loss: 0.345762, Acc: 0.904401\n",
      "[56/100] Loss: 0.343811, Acc: 0.905271\n",
      "[56/100] Loss: 0.344588, Acc: 0.904913\n",
      "Finish 56 epoch, Loss: 0.345367, Acc: 0.904900\n",
      "Test Loss: 0.328547, Acc: 0.910000\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 57\n",
      "[57/100] Loss: 0.341183, Acc: 0.905937\n",
      "[57/100] Loss: 0.339004, Acc: 0.906198\n",
      "[57/100] Loss: 0.344444, Acc: 0.905174\n",
      "[57/100] Loss: 0.343397, Acc: 0.905234\n",
      "[57/100] Loss: 0.343308, Acc: 0.905583\n",
      "[57/100] Loss: 0.344412, Acc: 0.905035\n",
      "Finish 57 epoch, Loss: 0.344397, Acc: 0.904950\n",
      "Test Loss: 0.327794, Acc: 0.910200\n",
      "Time: 8.2\n",
      "**********\n",
      "epoch 58\n",
      "[58/100] Loss: 0.345210, Acc: 0.903646\n",
      "[58/100] Loss: 0.342573, Acc: 0.906094\n",
      "[58/100] Loss: 0.345361, Acc: 0.904444\n",
      "[58/100] Loss: 0.345413, Acc: 0.904609\n",
      "[58/100] Loss: 0.343414, Acc: 0.905333\n",
      "[58/100] Loss: 0.343889, Acc: 0.904878\n",
      "Finish 58 epoch, Loss: 0.343419, Acc: 0.905083\n",
      "Test Loss: 0.326818, Acc: 0.910500\n",
      "Time: 7.4\n",
      "**********\n",
      "epoch 59\n",
      "[59/100] Loss: 0.358894, Acc: 0.901771\n",
      "[59/100] Loss: 0.358148, Acc: 0.900833\n",
      "[59/100] Loss: 0.348228, Acc: 0.903715\n",
      "[59/100] Loss: 0.347425, Acc: 0.903568\n",
      "[59/100] Loss: 0.344907, Acc: 0.904500\n",
      "[59/100] Loss: 0.342875, Acc: 0.904844\n",
      "Finish 59 epoch, Loss: 0.342520, Acc: 0.905283\n",
      "Test Loss: 0.326104, Acc: 0.910800\n",
      "Time: 7.4\n",
      "**********\n",
      "epoch 60\n",
      "[60/100] Loss: 0.337659, Acc: 0.909167\n",
      "[60/100] Loss: 0.343168, Acc: 0.907135\n",
      "[60/100] Loss: 0.343219, Acc: 0.905799\n",
      "[60/100] Loss: 0.342955, Acc: 0.906172\n",
      "[60/100] Loss: 0.341557, Acc: 0.905813\n",
      "[60/100] Loss: 0.341632, Acc: 0.905747\n",
      "Finish 60 epoch, Loss: 0.341604, Acc: 0.905750\n",
      "Test Loss: 0.325392, Acc: 0.910800\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 61\n",
      "[61/100] Loss: 0.332248, Acc: 0.912292\n",
      "[61/100] Loss: 0.327584, Acc: 0.910573\n",
      "[61/100] Loss: 0.336820, Acc: 0.907188\n",
      "[61/100] Loss: 0.337155, Acc: 0.907083\n",
      "[61/100] Loss: 0.338776, Acc: 0.906354\n",
      "[61/100] Loss: 0.339786, Acc: 0.906163\n",
      "Finish 61 epoch, Loss: 0.340725, Acc: 0.905850\n",
      "Test Loss: 0.324464, Acc: 0.911200\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 62\n",
      "[62/100] Loss: 0.337367, Acc: 0.908125\n",
      "[62/100] Loss: 0.342199, Acc: 0.906042\n",
      "[62/100] Loss: 0.340455, Acc: 0.906424\n",
      "[62/100] Loss: 0.339636, Acc: 0.906120\n",
      "[62/100] Loss: 0.341114, Acc: 0.905729\n",
      "[62/100] Loss: 0.339761, Acc: 0.906163\n",
      "Finish 62 epoch, Loss: 0.339871, Acc: 0.906150\n",
      "Test Loss: 0.323807, Acc: 0.911100\n",
      "Time: 8.2\n",
      "**********\n",
      "epoch 63\n",
      "[63/100] Loss: 0.339405, Acc: 0.903229\n",
      "[63/100] Loss: 0.345330, Acc: 0.904115\n",
      "[63/100] Loss: 0.346256, Acc: 0.904861\n",
      "[63/100] Loss: 0.345240, Acc: 0.905912\n",
      "[63/100] Loss: 0.343915, Acc: 0.905042\n",
      "[63/100] Loss: 0.339788, Acc: 0.906267\n",
      "Finish 63 epoch, Loss: 0.339034, Acc: 0.906367\n",
      "Test Loss: 0.323080, Acc: 0.911500\n",
      "Time: 8.9\n",
      "**********\n",
      "epoch 64\n",
      "[64/100] Loss: 0.351455, Acc: 0.900833\n",
      "[64/100] Loss: 0.341779, Acc: 0.903854\n",
      "[64/100] Loss: 0.341705, Acc: 0.905278\n",
      "[64/100] Loss: 0.340870, Acc: 0.905625\n",
      "[64/100] Loss: 0.339263, Acc: 0.905979\n",
      "[64/100] Loss: 0.338405, Acc: 0.906146\n",
      "Finish 64 epoch, Loss: 0.338217, Acc: 0.906433\n",
      "Test Loss: 0.322345, Acc: 0.911500\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 65\n",
      "[65/100] Loss: 0.346685, Acc: 0.905104\n",
      "[65/100] Loss: 0.342453, Acc: 0.906719\n",
      "[65/100] Loss: 0.339769, Acc: 0.907257\n",
      "[65/100] Loss: 0.340952, Acc: 0.905599\n",
      "[65/100] Loss: 0.337575, Acc: 0.906167\n",
      "[65/100] Loss: 0.336699, Acc: 0.906424\n",
      "Finish 65 epoch, Loss: 0.337418, Acc: 0.906533\n",
      "Test Loss: 0.321783, Acc: 0.912200\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 66\n",
      "[66/100] Loss: 0.341349, Acc: 0.907917\n",
      "[66/100] Loss: 0.331756, Acc: 0.909323\n",
      "[66/100] Loss: 0.333502, Acc: 0.908472\n",
      "[66/100] Loss: 0.335208, Acc: 0.906745\n",
      "[66/100] Loss: 0.337496, Acc: 0.906021\n",
      "[66/100] Loss: 0.336392, Acc: 0.906858\n",
      "Finish 66 epoch, Loss: 0.336630, Acc: 0.907017\n",
      "Test Loss: 0.321014, Acc: 0.912500\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 67\n",
      "[67/100] Loss: 0.335529, Acc: 0.908125\n",
      "[67/100] Loss: 0.333901, Acc: 0.906979\n",
      "[67/100] Loss: 0.336974, Acc: 0.906563\n",
      "[67/100] Loss: 0.332876, Acc: 0.907083\n",
      "[67/100] Loss: 0.333264, Acc: 0.907396\n",
      "[67/100] Loss: 0.336710, Acc: 0.906701\n",
      "Finish 67 epoch, Loss: 0.335874, Acc: 0.906867\n",
      "Test Loss: 0.320362, Acc: 0.912900\n",
      "Time: 9.4\n",
      "**********\n",
      "epoch 68\n",
      "[68/100] Loss: 0.328591, Acc: 0.905833\n",
      "[68/100] Loss: 0.328261, Acc: 0.907188\n",
      "[68/100] Loss: 0.330448, Acc: 0.908056\n",
      "[68/100] Loss: 0.331507, Acc: 0.907839\n",
      "[68/100] Loss: 0.337853, Acc: 0.906396\n",
      "[68/100] Loss: 0.335476, Acc: 0.907066\n",
      "Finish 68 epoch, Loss: 0.335128, Acc: 0.907233\n",
      "Test Loss: 0.319664, Acc: 0.912800\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 69\n",
      "[69/100] Loss: 0.334058, Acc: 0.907917\n",
      "[69/100] Loss: 0.328596, Acc: 0.908646\n",
      "[69/100] Loss: 0.331061, Acc: 0.908229\n",
      "[69/100] Loss: 0.332667, Acc: 0.907682\n",
      "[69/100] Loss: 0.333990, Acc: 0.907917\n",
      "[69/100] Loss: 0.335520, Acc: 0.907222\n",
      "Finish 69 epoch, Loss: 0.334395, Acc: 0.907267\n",
      "Test Loss: 0.319037, Acc: 0.913000\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 70\n",
      "[70/100] Loss: 0.328781, Acc: 0.910625\n",
      "[70/100] Loss: 0.333845, Acc: 0.909375\n",
      "[70/100] Loss: 0.336688, Acc: 0.906910\n",
      "[70/100] Loss: 0.334764, Acc: 0.906667\n",
      "[70/100] Loss: 0.336758, Acc: 0.906583\n",
      "[70/100] Loss: 0.334397, Acc: 0.907292\n",
      "Finish 70 epoch, Loss: 0.333677, Acc: 0.907467\n",
      "Test Loss: 0.318453, Acc: 0.912800\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 71\n",
      "[71/100] Loss: 0.323850, Acc: 0.912083\n",
      "[71/100] Loss: 0.323694, Acc: 0.912604\n",
      "[71/100] Loss: 0.327950, Acc: 0.910660\n",
      "[71/100] Loss: 0.331773, Acc: 0.909245\n",
      "[71/100] Loss: 0.332318, Acc: 0.908625\n",
      "[71/100] Loss: 0.332069, Acc: 0.908247\n",
      "Finish 71 epoch, Loss: 0.332967, Acc: 0.907733\n",
      "Test Loss: 0.317828, Acc: 0.912600\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 72\n",
      "[72/100] Loss: 0.326354, Acc: 0.910938\n",
      "[72/100] Loss: 0.337273, Acc: 0.907083\n",
      "[72/100] Loss: 0.337724, Acc: 0.905868\n",
      "[72/100] Loss: 0.334112, Acc: 0.906458\n",
      "[72/100] Loss: 0.335078, Acc: 0.906771\n",
      "[72/100] Loss: 0.333621, Acc: 0.907500\n",
      "Finish 72 epoch, Loss: 0.332291, Acc: 0.907950\n",
      "Test Loss: 0.317347, Acc: 0.913300\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 73\n",
      "[73/100] Loss: 0.336816, Acc: 0.907708\n",
      "[73/100] Loss: 0.332129, Acc: 0.910260\n",
      "[73/100] Loss: 0.327194, Acc: 0.910243\n",
      "[73/100] Loss: 0.327470, Acc: 0.909245\n",
      "[73/100] Loss: 0.328637, Acc: 0.909208\n",
      "[73/100] Loss: 0.332000, Acc: 0.907986\n",
      "Finish 73 epoch, Loss: 0.331622, Acc: 0.908000\n",
      "Test Loss: 0.316781, Acc: 0.912900\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 74\n",
      "[74/100] Loss: 0.334870, Acc: 0.905104\n",
      "[74/100] Loss: 0.335952, Acc: 0.907188\n",
      "[74/100] Loss: 0.336812, Acc: 0.906528\n",
      "[74/100] Loss: 0.335816, Acc: 0.906641\n",
      "[74/100] Loss: 0.332372, Acc: 0.907625\n",
      "[74/100] Loss: 0.331391, Acc: 0.908264\n",
      "Finish 74 epoch, Loss: 0.330947, Acc: 0.908200\n",
      "Test Loss: 0.316149, Acc: 0.913600\n",
      "Time: 9.4\n",
      "**********\n",
      "epoch 75\n",
      "[75/100] Loss: 0.342204, Acc: 0.905729\n",
      "[75/100] Loss: 0.338867, Acc: 0.905885\n",
      "[75/100] Loss: 0.336347, Acc: 0.906944\n",
      "[75/100] Loss: 0.335368, Acc: 0.906719\n",
      "[75/100] Loss: 0.332633, Acc: 0.907625\n",
      "[75/100] Loss: 0.330999, Acc: 0.908490\n",
      "Finish 75 epoch, Loss: 0.330329, Acc: 0.908683\n",
      "Test Loss: 0.315628, Acc: 0.913800\n",
      "Time: 9.8\n",
      "**********\n",
      "epoch 76\n",
      "[76/100] Loss: 0.329583, Acc: 0.910417\n",
      "[76/100] Loss: 0.326730, Acc: 0.909323\n",
      "[76/100] Loss: 0.325532, Acc: 0.909896\n",
      "[76/100] Loss: 0.326027, Acc: 0.909870\n",
      "[76/100] Loss: 0.328555, Acc: 0.909292\n",
      "[76/100] Loss: 0.330253, Acc: 0.908663\n",
      "Finish 76 epoch, Loss: 0.329693, Acc: 0.908717\n",
      "Test Loss: 0.315130, Acc: 0.913800\n",
      "Time: 9.2\n",
      "**********\n",
      "epoch 77\n",
      "[77/100] Loss: 0.334044, Acc: 0.908542\n",
      "[77/100] Loss: 0.325779, Acc: 0.910260\n",
      "[77/100] Loss: 0.327210, Acc: 0.909688\n",
      "[77/100] Loss: 0.329725, Acc: 0.909323\n",
      "[77/100] Loss: 0.328758, Acc: 0.908875\n",
      "[77/100] Loss: 0.328570, Acc: 0.908958\n",
      "Finish 77 epoch, Loss: 0.329068, Acc: 0.908950\n",
      "Test Loss: 0.314562, Acc: 0.914200\n",
      "Time: 9.3\n",
      "**********\n",
      "epoch 78\n",
      "[78/100] Loss: 0.330241, Acc: 0.909167\n",
      "[78/100] Loss: 0.329469, Acc: 0.908333\n",
      "[78/100] Loss: 0.330753, Acc: 0.908368\n",
      "[78/100] Loss: 0.329745, Acc: 0.908672\n",
      "[78/100] Loss: 0.329509, Acc: 0.909083\n",
      "[78/100] Loss: 0.328976, Acc: 0.908976\n",
      "Finish 78 epoch, Loss: 0.328467, Acc: 0.909133\n",
      "Test Loss: 0.314057, Acc: 0.914100\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 79\n",
      "[79/100] Loss: 0.324591, Acc: 0.907188\n",
      "[79/100] Loss: 0.327451, Acc: 0.908333\n",
      "[79/100] Loss: 0.325501, Acc: 0.908472\n",
      "[79/100] Loss: 0.325907, Acc: 0.908255\n",
      "[79/100] Loss: 0.328568, Acc: 0.908771\n",
      "[79/100] Loss: 0.327445, Acc: 0.909167\n",
      "Finish 79 epoch, Loss: 0.327859, Acc: 0.909333\n",
      "Test Loss: 0.313634, Acc: 0.914400\n",
      "Time: 8.0\n",
      "**********\n",
      "epoch 80\n",
      "[80/100] Loss: 0.336322, Acc: 0.906771\n",
      "[80/100] Loss: 0.335056, Acc: 0.908438\n",
      "[80/100] Loss: 0.331233, Acc: 0.909236\n",
      "[80/100] Loss: 0.329517, Acc: 0.909323\n",
      "[80/100] Loss: 0.328499, Acc: 0.909479\n",
      "[80/100] Loss: 0.326669, Acc: 0.909826\n",
      "Finish 80 epoch, Loss: 0.327273, Acc: 0.909467\n",
      "Test Loss: 0.313004, Acc: 0.914200\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 81\n",
      "[81/100] Loss: 0.318100, Acc: 0.910729\n",
      "[81/100] Loss: 0.326293, Acc: 0.909948\n",
      "[81/100] Loss: 0.325919, Acc: 0.910174\n",
      "[81/100] Loss: 0.327858, Acc: 0.909896\n",
      "[81/100] Loss: 0.325071, Acc: 0.910521\n",
      "[81/100] Loss: 0.326916, Acc: 0.909531\n",
      "Finish 81 epoch, Loss: 0.326681, Acc: 0.909500\n",
      "Test Loss: 0.312652, Acc: 0.914800\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 82\n",
      "[82/100] Loss: 0.336024, Acc: 0.906563\n",
      "[82/100] Loss: 0.336349, Acc: 0.906927\n",
      "[82/100] Loss: 0.331874, Acc: 0.907882\n",
      "[82/100] Loss: 0.328600, Acc: 0.909115\n",
      "[82/100] Loss: 0.328151, Acc: 0.909000\n",
      "[82/100] Loss: 0.327435, Acc: 0.909236\n",
      "Finish 82 epoch, Loss: 0.326154, Acc: 0.909617\n",
      "Test Loss: 0.312113, Acc: 0.914600\n",
      "Time: 9.7\n",
      "**********\n",
      "epoch 83\n",
      "[83/100] Loss: 0.326737, Acc: 0.908958\n",
      "[83/100] Loss: 0.329727, Acc: 0.908385\n",
      "[83/100] Loss: 0.326805, Acc: 0.909444\n",
      "[83/100] Loss: 0.327543, Acc: 0.908984\n",
      "[83/100] Loss: 0.325879, Acc: 0.909583\n",
      "[83/100] Loss: 0.324150, Acc: 0.909948\n",
      "Finish 83 epoch, Loss: 0.325579, Acc: 0.909617\n",
      "Test Loss: 0.311763, Acc: 0.915000\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 84\n",
      "[84/100] Loss: 0.329592, Acc: 0.909896\n",
      "[84/100] Loss: 0.329673, Acc: 0.909531\n",
      "[84/100] Loss: 0.327289, Acc: 0.910729\n",
      "[84/100] Loss: 0.327136, Acc: 0.910026\n",
      "[84/100] Loss: 0.325729, Acc: 0.910146\n",
      "[84/100] Loss: 0.326131, Acc: 0.909583\n",
      "Finish 84 epoch, Loss: 0.325042, Acc: 0.909750\n",
      "Test Loss: 0.311328, Acc: 0.915100\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 85\n",
      "[85/100] Loss: 0.344187, Acc: 0.906354\n",
      "[85/100] Loss: 0.335980, Acc: 0.908073\n",
      "[85/100] Loss: 0.329636, Acc: 0.909583\n",
      "[85/100] Loss: 0.329245, Acc: 0.909193\n",
      "[85/100] Loss: 0.325938, Acc: 0.909917\n",
      "[85/100] Loss: 0.325103, Acc: 0.909931\n",
      "Finish 85 epoch, Loss: 0.324511, Acc: 0.909950\n",
      "Test Loss: 0.310785, Acc: 0.914900\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 86\n",
      "[86/100] Loss: 0.322823, Acc: 0.910208\n",
      "[86/100] Loss: 0.322776, Acc: 0.910938\n",
      "[86/100] Loss: 0.323440, Acc: 0.910069\n",
      "[86/100] Loss: 0.321067, Acc: 0.910182\n",
      "[86/100] Loss: 0.320848, Acc: 0.910771\n",
      "[86/100] Loss: 0.323973, Acc: 0.910243\n",
      "Finish 86 epoch, Loss: 0.323964, Acc: 0.910200\n",
      "Test Loss: 0.310362, Acc: 0.915000\n",
      "Time: 8.6\n",
      "**********\n",
      "epoch 87\n",
      "[87/100] Loss: 0.323616, Acc: 0.910938\n",
      "[87/100] Loss: 0.317422, Acc: 0.914062\n",
      "[87/100] Loss: 0.323404, Acc: 0.911250\n",
      "[87/100] Loss: 0.321140, Acc: 0.910703\n",
      "[87/100] Loss: 0.322651, Acc: 0.910125\n",
      "[87/100] Loss: 0.324835, Acc: 0.909983\n",
      "Finish 87 epoch, Loss: 0.323470, Acc: 0.910133\n",
      "Test Loss: 0.309936, Acc: 0.915000\n",
      "Time: 9.5\n",
      "**********\n",
      "epoch 88\n",
      "[88/100] Loss: 0.334925, Acc: 0.908750\n",
      "[88/100] Loss: 0.327831, Acc: 0.909531\n",
      "[88/100] Loss: 0.329330, Acc: 0.908368\n",
      "[88/100] Loss: 0.328329, Acc: 0.908464\n",
      "[88/100] Loss: 0.323636, Acc: 0.909875\n",
      "[88/100] Loss: 0.322881, Acc: 0.910191\n",
      "Finish 88 epoch, Loss: 0.322958, Acc: 0.910183\n",
      "Test Loss: 0.309436, Acc: 0.914700\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 89\n",
      "[89/100] Loss: 0.320160, Acc: 0.911146\n",
      "[89/100] Loss: 0.323269, Acc: 0.911094\n",
      "[89/100] Loss: 0.326991, Acc: 0.910069\n",
      "[89/100] Loss: 0.324641, Acc: 0.910599\n",
      "[89/100] Loss: 0.322331, Acc: 0.910750\n",
      "[89/100] Loss: 0.322275, Acc: 0.910469\n",
      "Finish 89 epoch, Loss: 0.322459, Acc: 0.910367\n",
      "Test Loss: 0.309107, Acc: 0.915000\n",
      "Time: 7.9\n",
      "**********\n",
      "epoch 90\n",
      "[90/100] Loss: 0.333970, Acc: 0.906771\n",
      "[90/100] Loss: 0.326667, Acc: 0.907708\n",
      "[90/100] Loss: 0.321357, Acc: 0.910243\n",
      "[90/100] Loss: 0.320209, Acc: 0.910833\n",
      "[90/100] Loss: 0.322221, Acc: 0.910271\n",
      "[90/100] Loss: 0.322515, Acc: 0.910504\n",
      "Finish 90 epoch, Loss: 0.321964, Acc: 0.910433\n",
      "Test Loss: 0.308699, Acc: 0.915100\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 91\n",
      "[91/100] Loss: 0.338935, Acc: 0.902813\n",
      "[91/100] Loss: 0.324850, Acc: 0.907344\n",
      "[91/100] Loss: 0.323589, Acc: 0.909236\n",
      "[91/100] Loss: 0.325189, Acc: 0.909297\n",
      "[91/100] Loss: 0.326403, Acc: 0.908687\n",
      "[91/100] Loss: 0.322810, Acc: 0.910243\n",
      "Finish 91 epoch, Loss: 0.321481, Acc: 0.910550\n",
      "Test Loss: 0.308392, Acc: 0.914900\n",
      "Time: 9.8\n",
      "**********\n",
      "epoch 92\n",
      "[92/100] Loss: 0.331844, Acc: 0.910625\n",
      "[92/100] Loss: 0.326708, Acc: 0.909063\n",
      "[92/100] Loss: 0.322803, Acc: 0.910764\n",
      "[92/100] Loss: 0.320488, Acc: 0.911510\n",
      "[92/100] Loss: 0.322698, Acc: 0.910333\n",
      "[92/100] Loss: 0.320987, Acc: 0.910764\n",
      "Finish 92 epoch, Loss: 0.321012, Acc: 0.910850\n",
      "Test Loss: 0.307890, Acc: 0.914900\n",
      "Time: 7.9\n",
      "**********\n",
      "epoch 93\n",
      "[93/100] Loss: 0.309683, Acc: 0.911875\n",
      "[93/100] Loss: 0.319890, Acc: 0.910729\n",
      "[93/100] Loss: 0.319584, Acc: 0.911285\n",
      "[93/100] Loss: 0.320747, Acc: 0.910807\n",
      "[93/100] Loss: 0.319305, Acc: 0.910812\n",
      "[93/100] Loss: 0.320557, Acc: 0.910764\n",
      "Finish 93 epoch, Loss: 0.320539, Acc: 0.910850\n",
      "Test Loss: 0.307495, Acc: 0.915300\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 94\n",
      "[94/100] Loss: 0.312754, Acc: 0.915208\n",
      "[94/100] Loss: 0.313637, Acc: 0.915000\n",
      "[94/100] Loss: 0.318174, Acc: 0.912917\n",
      "[94/100] Loss: 0.318305, Acc: 0.912161\n",
      "[94/100] Loss: 0.318619, Acc: 0.911667\n",
      "[94/100] Loss: 0.320338, Acc: 0.910990\n",
      "Finish 94 epoch, Loss: 0.320079, Acc: 0.910983\n",
      "Test Loss: 0.307112, Acc: 0.915100\n",
      "Time: 7.3\n",
      "**********\n",
      "epoch 95\n",
      "[95/100] Loss: 0.324232, Acc: 0.910313\n",
      "[95/100] Loss: 0.323595, Acc: 0.910573\n",
      "[95/100] Loss: 0.321342, Acc: 0.911111\n",
      "[95/100] Loss: 0.317606, Acc: 0.911771\n",
      "[95/100] Loss: 0.319578, Acc: 0.911354\n",
      "[95/100] Loss: 0.319035, Acc: 0.911233\n",
      "Finish 95 epoch, Loss: 0.319628, Acc: 0.911017\n",
      "Test Loss: 0.306687, Acc: 0.915300\n",
      "Time: 9.7\n",
      "**********\n",
      "epoch 96\n",
      "[96/100] Loss: 0.319731, Acc: 0.910313\n",
      "[96/100] Loss: 0.323925, Acc: 0.907396\n",
      "[96/100] Loss: 0.321702, Acc: 0.910278\n",
      "[96/100] Loss: 0.317227, Acc: 0.911589\n",
      "[96/100] Loss: 0.318719, Acc: 0.911583\n",
      "[96/100] Loss: 0.319244, Acc: 0.911337\n",
      "Finish 96 epoch, Loss: 0.319173, Acc: 0.911350\n",
      "Test Loss: 0.306379, Acc: 0.915900\n",
      "Time: 7.2\n",
      "**********\n",
      "epoch 97\n",
      "[97/100] Loss: 0.316207, Acc: 0.911979\n",
      "[97/100] Loss: 0.312243, Acc: 0.912292\n",
      "[97/100] Loss: 0.310721, Acc: 0.913715\n",
      "[97/100] Loss: 0.317419, Acc: 0.911276\n",
      "[97/100] Loss: 0.317839, Acc: 0.911271\n",
      "[97/100] Loss: 0.319472, Acc: 0.910972\n",
      "Finish 97 epoch, Loss: 0.318733, Acc: 0.911233\n",
      "Test Loss: 0.306061, Acc: 0.915600\n",
      "Time: 8.8\n",
      "**********\n",
      "epoch 98\n",
      "[98/100] Loss: 0.322311, Acc: 0.910417\n",
      "[98/100] Loss: 0.322520, Acc: 0.909479\n",
      "[98/100] Loss: 0.321270, Acc: 0.909861\n",
      "[98/100] Loss: 0.322168, Acc: 0.909792\n",
      "[98/100] Loss: 0.320807, Acc: 0.910937\n",
      "[98/100] Loss: 0.318440, Acc: 0.911701\n",
      "Finish 98 epoch, Loss: 0.318304, Acc: 0.911433\n",
      "Test Loss: 0.305706, Acc: 0.915700\n",
      "Time: 9.1\n",
      "**********\n",
      "epoch 99\n",
      "[99/100] Loss: 0.315614, Acc: 0.909792\n",
      "[99/100] Loss: 0.315976, Acc: 0.910208\n",
      "[99/100] Loss: 0.321658, Acc: 0.909826\n",
      "[99/100] Loss: 0.318674, Acc: 0.910391\n",
      "[99/100] Loss: 0.316127, Acc: 0.911396\n",
      "[99/100] Loss: 0.317580, Acc: 0.911858\n",
      "Finish 99 epoch, Loss: 0.317875, Acc: 0.911567\n",
      "Test Loss: 0.305381, Acc: 0.915900\n",
      "Time: 7.1\n",
      "**********\n",
      "epoch 100\n",
      "[100/100] Loss: 0.325376, Acc: 0.906250\n",
      "[100/100] Loss: 0.315333, Acc: 0.910781\n",
      "[100/100] Loss: 0.311115, Acc: 0.912118\n",
      "[100/100] Loss: 0.314258, Acc: 0.911328\n",
      "[100/100] Loss: 0.316470, Acc: 0.911187\n",
      "[100/100] Loss: 0.317469, Acc: 0.911441\n",
      "Finish 100 epoch, Loss: 0.317452, Acc: 0.911567\n",
      "Test Loss: 0.304948, Acc: 0.915900\n",
      "Time: 9.3\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    print('*' * 10)\n",
    "    print(f'epoch {epoch+1}')\n",
    "    since = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1) # 将图片展开成 28x28\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        # forward propagation\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum().float()\n",
    "        running_acc += num_correct\n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 300 == 0:\n",
    "            print(f'[{epoch+1}/{num_epoches}] Loss: {running_loss/(batch_size*i):.6f}, Acc: {running_acc/(batch_size*i):.6f}')\n",
    "    print(f'Finish {epoch+1} epoch, Loss: {running_loss/(len(train_dataset)):.6f}, Acc: {running_acc/len(train_dataset):.6f}')\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_acc = 0.0\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum().float()\n",
    "        eval_acc += num_correct\n",
    "    print(f'Test Loss: {eval_loss/(len(test_dataset)):.6f}, Acc: {eval_acc/(len(test_dataset)):.6f}')\n",
    "    print(f'Time: {time.time() - since:.1f}')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'save/02-logistic regression.pytorch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
