{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "2018-04-25：更新为新版本代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # whether GPU is supportted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入/下载训练集 MNIST 手写数字训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('../alldata/mnist', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST('../alldata/mnist', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 Logistic Regression 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(Logistic_Regression, self).__init__()\n",
    "        self.logistic = nn.Linear(in_dim, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.logistic(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logistic_Regression(28 * 28, 10).to(device) # pictures' size are 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑斯蒂回归与线性回归的区别可以在 loss 函数的定义上，Logistic 回归的损失函数为交叉熵损失函数，即\n",
    "\n",
    "$$L_H(\\mathbf x,\\mathbf z)=-\\sum_{k=1}^dx_k\\log z_k+(1-x_k)\\log(1-z_k).$$\n",
    "\n",
    "优化器与前者一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "[1/100] Loss: 2.205960, Acc: 0.261562\n",
      "[1/100] Loss: 2.073385, Acc: 0.424740\n",
      "[1/100] Loss: 1.959040, Acc: 0.516771\n",
      "[1/100] Loss: 1.859974, Acc: 0.575417\n",
      "[1/100] Loss: 1.772717, Acc: 0.614563\n",
      "[1/100] Loss: 1.695495, Acc: 0.644896\n",
      "Finish 1 epoch, Loss: 1.677900, Acc: 0.650633\n",
      "Test Loss: 1.225696, Acc: 0.802800\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 2\n",
      "[2/100] Loss: 1.204241, Acc: 0.800208\n",
      "[2/100] Loss: 1.167027, Acc: 0.803073\n",
      "[2/100] Loss: 1.135918, Acc: 0.806458\n",
      "[2/100] Loss: 1.108320, Acc: 0.808255\n",
      "[2/100] Loss: 1.083039, Acc: 0.810750\n",
      "[2/100] Loss: 1.058421, Acc: 0.813142\n",
      "Finish 2 epoch, Loss: 1.052422, Acc: 0.813767\n",
      "Test Loss: 0.890437, Acc: 0.832700\n",
      "Time: 4.8\n",
      "**********\n",
      "epoch 3\n",
      "[3/100] Loss: 0.887132, Acc: 0.827917\n",
      "[3/100] Loss: 0.880385, Acc: 0.827917\n",
      "[3/100] Loss: 0.868709, Acc: 0.828785\n",
      "[3/100] Loss: 0.855259, Acc: 0.830729\n",
      "[3/100] Loss: 0.845744, Acc: 0.831292\n",
      "[3/100] Loss: 0.835769, Acc: 0.832622\n",
      "Finish 3 epoch, Loss: 0.832114, Acc: 0.833317\n",
      "Test Loss: 0.742263, Acc: 0.848900\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 4\n",
      "[4/100] Loss: 0.754807, Acc: 0.845729\n",
      "[4/100] Loss: 0.745866, Acc: 0.846563\n",
      "[4/100] Loss: 0.740279, Acc: 0.844931\n",
      "[4/100] Loss: 0.735348, Acc: 0.844427\n",
      "[4/100] Loss: 0.726698, Acc: 0.846313\n",
      "[4/100] Loss: 0.723023, Acc: 0.845990\n",
      "Finish 4 epoch, Loss: 0.721303, Acc: 0.846033\n",
      "Test Loss: 0.657972, Acc: 0.858900\n",
      "Time: 4.8\n",
      "**********\n",
      "epoch 5\n",
      "[5/100] Loss: 0.679066, Acc: 0.844479\n",
      "[5/100] Loss: 0.672483, Acc: 0.848958\n",
      "[5/100] Loss: 0.668854, Acc: 0.850625\n",
      "[5/100] Loss: 0.665270, Acc: 0.851354\n",
      "[5/100] Loss: 0.659946, Acc: 0.852500\n",
      "[5/100] Loss: 0.654176, Acc: 0.853854\n",
      "Finish 5 epoch, Loss: 0.653486, Acc: 0.853633\n",
      "Test Loss: 0.603329, Acc: 0.868700\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 6\n",
      "[6/100] Loss: 0.629413, Acc: 0.853021\n",
      "[6/100] Loss: 0.618006, Acc: 0.857188\n",
      "[6/100] Loss: 0.616688, Acc: 0.857535\n",
      "[6/100] Loss: 0.615771, Acc: 0.857995\n",
      "[6/100] Loss: 0.611384, Acc: 0.858771\n",
      "[6/100] Loss: 0.608733, Acc: 0.858715\n",
      "Finish 6 epoch, Loss: 0.607093, Acc: 0.859117\n",
      "Test Loss: 0.564199, Acc: 0.873200\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 7\n",
      "[7/100] Loss: 0.584158, Acc: 0.864688\n",
      "[7/100] Loss: 0.584413, Acc: 0.863646\n",
      "[7/100] Loss: 0.579164, Acc: 0.864375\n",
      "[7/100] Loss: 0.576273, Acc: 0.864661\n",
      "[7/100] Loss: 0.574311, Acc: 0.864542\n",
      "[7/100] Loss: 0.572182, Acc: 0.864462\n",
      "Finish 7 epoch, Loss: 0.573031, Acc: 0.863900\n",
      "Test Loss: 0.534824, Acc: 0.875100\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 8\n",
      "[8/100] Loss: 0.552002, Acc: 0.868750\n",
      "[8/100] Loss: 0.555129, Acc: 0.867396\n",
      "[8/100] Loss: 0.551558, Acc: 0.866458\n",
      "[8/100] Loss: 0.548089, Acc: 0.867708\n",
      "[8/100] Loss: 0.546892, Acc: 0.867917\n",
      "[8/100] Loss: 0.547489, Acc: 0.867639\n",
      "Finish 8 epoch, Loss: 0.546746, Acc: 0.867617\n",
      "Test Loss: 0.511793, Acc: 0.878500\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 9\n",
      "[9/100] Loss: 0.542227, Acc: 0.866875\n",
      "[9/100] Loss: 0.532630, Acc: 0.870417\n",
      "[9/100] Loss: 0.532321, Acc: 0.870139\n",
      "[9/100] Loss: 0.528937, Acc: 0.870443\n",
      "[9/100] Loss: 0.526626, Acc: 0.870583\n",
      "[9/100] Loss: 0.525954, Acc: 0.870226\n",
      "Finish 9 epoch, Loss: 0.525720, Acc: 0.870450\n",
      "Test Loss: 0.493067, Acc: 0.881600\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 10\n",
      "[10/100] Loss: 0.516152, Acc: 0.869687\n",
      "[10/100] Loss: 0.517466, Acc: 0.868958\n",
      "[10/100] Loss: 0.514266, Acc: 0.870000\n",
      "[10/100] Loss: 0.515542, Acc: 0.870573\n",
      "[10/100] Loss: 0.513388, Acc: 0.871625\n",
      "[10/100] Loss: 0.510372, Acc: 0.872517\n",
      "Finish 10 epoch, Loss: 0.508424, Acc: 0.873200\n",
      "Test Loss: 0.477572, Acc: 0.883100\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 11\n",
      "[11/100] Loss: 0.489502, Acc: 0.881875\n",
      "[11/100] Loss: 0.496654, Acc: 0.876667\n",
      "[11/100] Loss: 0.496316, Acc: 0.876007\n",
      "[11/100] Loss: 0.496365, Acc: 0.875104\n",
      "[11/100] Loss: 0.494602, Acc: 0.876104\n",
      "[11/100] Loss: 0.494309, Acc: 0.875955\n",
      "Finish 11 epoch, Loss: 0.493920, Acc: 0.875833\n",
      "Test Loss: 0.464344, Acc: 0.885000\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 12\n",
      "[12/100] Loss: 0.493905, Acc: 0.875833\n",
      "[12/100] Loss: 0.490102, Acc: 0.875990\n",
      "[12/100] Loss: 0.491576, Acc: 0.874410\n",
      "[12/100] Loss: 0.486920, Acc: 0.876641\n",
      "[12/100] Loss: 0.483350, Acc: 0.877375\n",
      "[12/100] Loss: 0.481793, Acc: 0.877865\n",
      "Finish 12 epoch, Loss: 0.481488, Acc: 0.877867\n",
      "Test Loss: 0.453110, Acc: 0.886300\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 13\n",
      "[13/100] Loss: 0.474195, Acc: 0.880000\n",
      "[13/100] Loss: 0.476104, Acc: 0.878958\n",
      "[13/100] Loss: 0.473453, Acc: 0.879549\n",
      "[13/100] Loss: 0.471838, Acc: 0.880911\n",
      "[13/100] Loss: 0.471574, Acc: 0.879812\n",
      "[13/100] Loss: 0.471076, Acc: 0.879809\n",
      "Finish 13 epoch, Loss: 0.470730, Acc: 0.879683\n",
      "Test Loss: 0.443223, Acc: 0.888100\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 14\n",
      "[14/100] Loss: 0.461679, Acc: 0.880938\n",
      "[14/100] Loss: 0.460581, Acc: 0.880208\n",
      "[14/100] Loss: 0.461062, Acc: 0.878750\n",
      "[14/100] Loss: 0.459565, Acc: 0.880391\n",
      "[14/100] Loss: 0.460125, Acc: 0.881083\n",
      "[14/100] Loss: 0.460884, Acc: 0.881302\n",
      "Finish 14 epoch, Loss: 0.461270, Acc: 0.881217\n",
      "Test Loss: 0.434605, Acc: 0.890300\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 15\n",
      "[15/100] Loss: 0.450829, Acc: 0.881458\n",
      "[15/100] Loss: 0.445572, Acc: 0.884844\n",
      "[15/100] Loss: 0.447684, Acc: 0.884271\n",
      "[15/100] Loss: 0.450503, Acc: 0.883464\n",
      "[15/100] Loss: 0.452464, Acc: 0.883083\n",
      "[15/100] Loss: 0.452260, Acc: 0.882726\n",
      "Finish 15 epoch, Loss: 0.452897, Acc: 0.882383\n",
      "Test Loss: 0.426975, Acc: 0.891500\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 16\n",
      "[16/100] Loss: 0.446121, Acc: 0.882292\n",
      "[16/100] Loss: 0.444925, Acc: 0.883490\n",
      "[16/100] Loss: 0.447932, Acc: 0.883021\n",
      "[16/100] Loss: 0.444074, Acc: 0.884193\n",
      "[16/100] Loss: 0.445180, Acc: 0.883500\n",
      "[16/100] Loss: 0.445438, Acc: 0.883715\n",
      "Finish 16 epoch, Loss: 0.445387, Acc: 0.883800\n",
      "Test Loss: 0.419987, Acc: 0.892000\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 17\n",
      "[17/100] Loss: 0.437735, Acc: 0.885417\n",
      "[17/100] Loss: 0.438388, Acc: 0.883958\n",
      "[17/100] Loss: 0.443402, Acc: 0.882431\n",
      "[17/100] Loss: 0.441778, Acc: 0.883802\n",
      "[17/100] Loss: 0.440139, Acc: 0.884479\n",
      "[17/100] Loss: 0.439526, Acc: 0.884288\n",
      "Finish 17 epoch, Loss: 0.438633, Acc: 0.884817\n",
      "Test Loss: 0.413702, Acc: 0.893000\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 18\n",
      "[18/100] Loss: 0.426934, Acc: 0.893750\n",
      "[18/100] Loss: 0.429303, Acc: 0.888021\n",
      "[18/100] Loss: 0.431436, Acc: 0.886979\n",
      "[18/100] Loss: 0.429701, Acc: 0.887708\n",
      "[18/100] Loss: 0.432301, Acc: 0.886521\n",
      "[18/100] Loss: 0.432284, Acc: 0.886493\n",
      "Finish 18 epoch, Loss: 0.432486, Acc: 0.886367\n",
      "Test Loss: 0.408105, Acc: 0.894400\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 19\n",
      "[19/100] Loss: 0.433281, Acc: 0.885625\n",
      "[19/100] Loss: 0.424861, Acc: 0.887396\n",
      "[19/100] Loss: 0.425336, Acc: 0.887639\n",
      "[19/100] Loss: 0.427480, Acc: 0.886589\n",
      "[19/100] Loss: 0.427741, Acc: 0.887292\n",
      "[19/100] Loss: 0.427651, Acc: 0.887187\n",
      "Finish 19 epoch, Loss: 0.426896, Acc: 0.887383\n",
      "Test Loss: 0.402932, Acc: 0.895200\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 20\n",
      "[20/100] Loss: 0.418603, Acc: 0.889167\n",
      "[20/100] Loss: 0.421090, Acc: 0.889844\n",
      "[20/100] Loss: 0.422555, Acc: 0.887639\n",
      "[20/100] Loss: 0.420523, Acc: 0.888021\n",
      "[20/100] Loss: 0.422717, Acc: 0.887417\n",
      "[20/100] Loss: 0.421844, Acc: 0.888021\n",
      "Finish 20 epoch, Loss: 0.421748, Acc: 0.888033\n",
      "Test Loss: 0.398171, Acc: 0.896600\n",
      "Time: 4.8\n",
      "**********\n",
      "epoch 21\n",
      "[21/100] Loss: 0.420090, Acc: 0.892708\n",
      "[21/100] Loss: 0.421452, Acc: 0.889740\n",
      "[21/100] Loss: 0.421640, Acc: 0.888785\n",
      "[21/100] Loss: 0.419483, Acc: 0.889089\n",
      "[21/100] Loss: 0.416259, Acc: 0.889604\n",
      "[21/100] Loss: 0.416957, Acc: 0.889462\n",
      "Finish 21 epoch, Loss: 0.417035, Acc: 0.889367\n",
      "Test Loss: 0.393956, Acc: 0.896600\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 22\n",
      "[22/100] Loss: 0.415722, Acc: 0.888854\n",
      "[22/100] Loss: 0.422925, Acc: 0.886510\n",
      "[22/100] Loss: 0.419629, Acc: 0.886979\n",
      "[22/100] Loss: 0.416241, Acc: 0.888229\n",
      "[22/100] Loss: 0.413795, Acc: 0.889604\n",
      "[22/100] Loss: 0.411962, Acc: 0.890191\n",
      "Finish 22 epoch, Loss: 0.412634, Acc: 0.890083\n",
      "Test Loss: 0.389978, Acc: 0.897300\n",
      "Time: 4.5\n",
      "**********\n",
      "epoch 23\n",
      "[23/100] Loss: 0.414607, Acc: 0.887292\n",
      "[23/100] Loss: 0.411262, Acc: 0.892083\n",
      "[23/100] Loss: 0.411337, Acc: 0.890972\n",
      "[23/100] Loss: 0.413121, Acc: 0.889505\n",
      "[23/100] Loss: 0.411572, Acc: 0.889771\n",
      "[23/100] Loss: 0.408523, Acc: 0.891007\n",
      "Finish 23 epoch, Loss: 0.408547, Acc: 0.891083\n",
      "Test Loss: 0.386231, Acc: 0.897500\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 24\n",
      "[24/100] Loss: 0.401945, Acc: 0.892292\n",
      "[24/100] Loss: 0.405955, Acc: 0.890208\n",
      "[24/100] Loss: 0.404073, Acc: 0.890764\n",
      "[24/100] Loss: 0.402223, Acc: 0.892031\n",
      "[24/100] Loss: 0.404390, Acc: 0.892271\n",
      "[24/100] Loss: 0.404035, Acc: 0.891997\n",
      "Finish 24 epoch, Loss: 0.404778, Acc: 0.891583\n",
      "Test Loss: 0.382706, Acc: 0.898700\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 25\n",
      "[25/100] Loss: 0.402248, Acc: 0.889896\n",
      "[25/100] Loss: 0.401808, Acc: 0.891094\n",
      "[25/100] Loss: 0.398862, Acc: 0.892882\n",
      "[25/100] Loss: 0.401965, Acc: 0.892760\n",
      "[25/100] Loss: 0.401180, Acc: 0.892854\n",
      "[25/100] Loss: 0.400355, Acc: 0.893142\n",
      "Finish 25 epoch, Loss: 0.401198, Acc: 0.892500\n",
      "Test Loss: 0.379289, Acc: 0.899200\n",
      "Time: 4.5\n",
      "**********\n",
      "epoch 26\n",
      "[26/100] Loss: 0.392982, Acc: 0.892292\n",
      "[26/100] Loss: 0.401410, Acc: 0.889010\n",
      "[26/100] Loss: 0.395889, Acc: 0.892326\n",
      "[26/100] Loss: 0.398723, Acc: 0.892240\n",
      "[26/100] Loss: 0.395427, Acc: 0.893271\n",
      "[26/100] Loss: 0.397550, Acc: 0.893229\n",
      "Finish 26 epoch, Loss: 0.397869, Acc: 0.893000\n",
      "Test Loss: 0.376398, Acc: 0.900200\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 27\n",
      "[27/100] Loss: 0.400093, Acc: 0.890104\n",
      "[27/100] Loss: 0.396346, Acc: 0.892292\n",
      "[27/100] Loss: 0.394974, Acc: 0.891944\n",
      "[27/100] Loss: 0.396017, Acc: 0.892240\n",
      "[27/100] Loss: 0.392744, Acc: 0.893438\n",
      "[27/100] Loss: 0.395044, Acc: 0.893368\n",
      "Finish 27 epoch, Loss: 0.394714, Acc: 0.893533\n",
      "Test Loss: 0.373426, Acc: 0.900600\n",
      "Time: 4.7\n",
      "**********\n",
      "epoch 28\n",
      "[28/100] Loss: 0.400280, Acc: 0.895729\n",
      "[28/100] Loss: 0.394710, Acc: 0.895833\n",
      "[28/100] Loss: 0.395194, Acc: 0.894653\n",
      "[28/100] Loss: 0.391946, Acc: 0.894844\n",
      "[28/100] Loss: 0.393288, Acc: 0.893812\n",
      "[28/100] Loss: 0.392985, Acc: 0.893559\n",
      "Finish 28 epoch, Loss: 0.391771, Acc: 0.894050\n",
      "Test Loss: 0.370795, Acc: 0.901800\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 29\n",
      "[29/100] Loss: 0.378605, Acc: 0.900312\n",
      "[29/100] Loss: 0.388199, Acc: 0.895833\n",
      "[29/100] Loss: 0.387421, Acc: 0.895694\n",
      "[29/100] Loss: 0.387243, Acc: 0.895312\n",
      "[29/100] Loss: 0.388697, Acc: 0.894667\n",
      "[29/100] Loss: 0.387617, Acc: 0.895035\n",
      "Finish 29 epoch, Loss: 0.388965, Acc: 0.894750\n",
      "Test Loss: 0.368203, Acc: 0.902300\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 30\n",
      "[30/100] Loss: 0.397438, Acc: 0.891667\n",
      "[30/100] Loss: 0.389298, Acc: 0.895000\n",
      "[30/100] Loss: 0.387128, Acc: 0.894861\n",
      "[30/100] Loss: 0.387495, Acc: 0.894922\n",
      "[30/100] Loss: 0.385688, Acc: 0.895771\n",
      "[30/100] Loss: 0.386828, Acc: 0.895243\n",
      "Finish 30 epoch, Loss: 0.386332, Acc: 0.895467\n",
      "Test Loss: 0.365770, Acc: 0.902900\n",
      "Time: 4.8\n",
      "**********\n",
      "epoch 31\n",
      "[31/100] Loss: 0.394450, Acc: 0.894167\n",
      "[31/100] Loss: 0.383718, Acc: 0.897552\n",
      "[31/100] Loss: 0.381491, Acc: 0.898021\n",
      "[31/100] Loss: 0.383939, Acc: 0.896979\n",
      "[31/100] Loss: 0.383765, Acc: 0.896229\n",
      "[31/100] Loss: 0.384005, Acc: 0.895816\n",
      "Finish 31 epoch, Loss: 0.383804, Acc: 0.896200\n",
      "Test Loss: 0.363491, Acc: 0.903500\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 32\n",
      "[32/100] Loss: 0.382141, Acc: 0.893958\n",
      "[32/100] Loss: 0.385780, Acc: 0.895833\n",
      "[32/100] Loss: 0.384259, Acc: 0.895208\n",
      "[32/100] Loss: 0.381594, Acc: 0.896458\n",
      "[32/100] Loss: 0.380866, Acc: 0.896333\n",
      "[32/100] Loss: 0.382029, Acc: 0.896267\n",
      "Finish 32 epoch, Loss: 0.381411, Acc: 0.896483\n",
      "Test Loss: 0.361194, Acc: 0.903500\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 33\n",
      "[33/100] Loss: 0.379514, Acc: 0.896979\n",
      "[33/100] Loss: 0.381219, Acc: 0.896354\n",
      "[33/100] Loss: 0.380725, Acc: 0.896597\n",
      "[33/100] Loss: 0.378029, Acc: 0.897943\n",
      "[33/100] Loss: 0.377178, Acc: 0.898271\n",
      "[33/100] Loss: 0.378893, Acc: 0.897153\n",
      "Finish 33 epoch, Loss: 0.379146, Acc: 0.897267\n",
      "Test Loss: 0.359201, Acc: 0.903900\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 34\n",
      "[34/100] Loss: 0.371714, Acc: 0.899063\n",
      "[34/100] Loss: 0.373361, Acc: 0.899167\n",
      "[34/100] Loss: 0.375626, Acc: 0.899028\n",
      "[34/100] Loss: 0.377144, Acc: 0.897135\n",
      "[34/100] Loss: 0.377380, Acc: 0.897104\n",
      "[34/100] Loss: 0.376835, Acc: 0.897361\n",
      "Finish 34 epoch, Loss: 0.376959, Acc: 0.897633\n",
      "Test Loss: 0.357131, Acc: 0.904700\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 35\n",
      "[35/100] Loss: 0.372241, Acc: 0.900104\n",
      "[35/100] Loss: 0.375630, Acc: 0.897708\n",
      "[35/100] Loss: 0.373098, Acc: 0.899028\n",
      "[35/100] Loss: 0.377254, Acc: 0.897865\n",
      "[35/100] Loss: 0.377257, Acc: 0.897625\n",
      "[35/100] Loss: 0.375358, Acc: 0.897934\n",
      "Finish 35 epoch, Loss: 0.374890, Acc: 0.898083\n",
      "Test Loss: 0.355348, Acc: 0.904900\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 36\n",
      "[36/100] Loss: 0.378734, Acc: 0.896042\n",
      "[36/100] Loss: 0.374095, Acc: 0.898490\n",
      "[36/100] Loss: 0.374325, Acc: 0.898021\n",
      "[36/100] Loss: 0.372433, Acc: 0.898490\n",
      "[36/100] Loss: 0.373194, Acc: 0.898750\n",
      "[36/100] Loss: 0.372310, Acc: 0.898924\n",
      "Finish 36 epoch, Loss: 0.372909, Acc: 0.898450\n",
      "Test Loss: 0.353572, Acc: 0.905000\n",
      "Time: 5.3\n",
      "**********\n",
      "epoch 37\n",
      "[37/100] Loss: 0.367066, Acc: 0.901250\n",
      "[37/100] Loss: 0.366314, Acc: 0.900469\n",
      "[37/100] Loss: 0.373599, Acc: 0.898611\n",
      "[37/100] Loss: 0.373823, Acc: 0.897786\n",
      "[37/100] Loss: 0.371680, Acc: 0.898479\n",
      "[37/100] Loss: 0.370236, Acc: 0.898958\n",
      "Finish 37 epoch, Loss: 0.371007, Acc: 0.898833\n",
      "Test Loss: 0.351863, Acc: 0.905200\n",
      "Time: 5.3\n",
      "**********\n",
      "epoch 38\n",
      "[38/100] Loss: 0.371095, Acc: 0.899375\n",
      "[38/100] Loss: 0.373608, Acc: 0.897969\n",
      "[38/100] Loss: 0.367404, Acc: 0.900625\n",
      "[38/100] Loss: 0.367626, Acc: 0.900286\n",
      "[38/100] Loss: 0.368636, Acc: 0.900042\n",
      "[38/100] Loss: 0.368631, Acc: 0.899583\n",
      "Finish 38 epoch, Loss: 0.369181, Acc: 0.899283\n",
      "Test Loss: 0.350143, Acc: 0.905600\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 39\n",
      "[39/100] Loss: 0.360707, Acc: 0.903125\n",
      "[39/100] Loss: 0.363941, Acc: 0.901146\n",
      "[39/100] Loss: 0.365994, Acc: 0.899410\n",
      "[39/100] Loss: 0.362973, Acc: 0.900000\n",
      "[39/100] Loss: 0.365026, Acc: 0.900021\n",
      "[39/100] Loss: 0.367989, Acc: 0.899427\n",
      "Finish 39 epoch, Loss: 0.367431, Acc: 0.899783\n",
      "Test Loss: 0.348654, Acc: 0.905800\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 40\n",
      "[40/100] Loss: 0.376538, Acc: 0.896563\n",
      "[40/100] Loss: 0.367083, Acc: 0.899635\n",
      "[40/100] Loss: 0.366935, Acc: 0.899410\n",
      "[40/100] Loss: 0.367253, Acc: 0.899271\n",
      "[40/100] Loss: 0.367382, Acc: 0.899375\n",
      "[40/100] Loss: 0.365724, Acc: 0.900069\n",
      "Finish 40 epoch, Loss: 0.365730, Acc: 0.900133\n",
      "Test Loss: 0.347044, Acc: 0.906000\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 41\n",
      "[41/100] Loss: 0.365924, Acc: 0.898229\n",
      "[41/100] Loss: 0.361646, Acc: 0.901198\n",
      "[41/100] Loss: 0.363394, Acc: 0.902326\n",
      "[41/100] Loss: 0.363992, Acc: 0.901484\n",
      "[41/100] Loss: 0.365333, Acc: 0.900521\n",
      "[41/100] Loss: 0.364874, Acc: 0.900556\n",
      "Finish 41 epoch, Loss: 0.364123, Acc: 0.900600\n",
      "Test Loss: 0.345534, Acc: 0.906500\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 42\n",
      "[42/100] Loss: 0.362823, Acc: 0.899271\n",
      "[42/100] Loss: 0.358875, Acc: 0.901979\n",
      "[42/100] Loss: 0.360536, Acc: 0.901493\n",
      "[42/100] Loss: 0.364170, Acc: 0.900573\n",
      "[42/100] Loss: 0.364646, Acc: 0.900146\n",
      "[42/100] Loss: 0.362707, Acc: 0.900955\n",
      "Finish 42 epoch, Loss: 0.362563, Acc: 0.901150\n",
      "Test Loss: 0.344225, Acc: 0.906900\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 43\n",
      "[43/100] Loss: 0.377139, Acc: 0.895729\n",
      "[43/100] Loss: 0.364822, Acc: 0.899427\n",
      "[43/100] Loss: 0.361754, Acc: 0.899618\n",
      "[43/100] Loss: 0.361902, Acc: 0.899896\n",
      "[43/100] Loss: 0.358759, Acc: 0.901521\n",
      "[43/100] Loss: 0.361308, Acc: 0.901389\n",
      "Finish 43 epoch, Loss: 0.361059, Acc: 0.901467\n",
      "Test Loss: 0.342857, Acc: 0.907200\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 44\n",
      "[44/100] Loss: 0.360319, Acc: 0.900104\n",
      "[44/100] Loss: 0.356221, Acc: 0.903021\n",
      "[44/100] Loss: 0.360118, Acc: 0.901181\n",
      "[44/100] Loss: 0.358777, Acc: 0.902240\n",
      "[44/100] Loss: 0.360974, Acc: 0.901396\n",
      "[44/100] Loss: 0.359813, Acc: 0.901701\n",
      "Finish 44 epoch, Loss: 0.359606, Acc: 0.901817\n",
      "Test Loss: 0.341517, Acc: 0.907800\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 45\n",
      "[45/100] Loss: 0.377767, Acc: 0.894687\n",
      "[45/100] Loss: 0.369825, Acc: 0.898021\n",
      "[45/100] Loss: 0.362838, Acc: 0.899549\n",
      "[45/100] Loss: 0.360355, Acc: 0.901458\n",
      "[45/100] Loss: 0.358842, Acc: 0.902042\n",
      "[45/100] Loss: 0.358470, Acc: 0.902344\n",
      "Finish 45 epoch, Loss: 0.358202, Acc: 0.902117\n",
      "Test Loss: 0.340274, Acc: 0.907400\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 46\n",
      "[46/100] Loss: 0.361846, Acc: 0.903646\n",
      "[46/100] Loss: 0.363446, Acc: 0.900573\n",
      "[46/100] Loss: 0.361567, Acc: 0.900521\n",
      "[46/100] Loss: 0.362010, Acc: 0.900651\n",
      "[46/100] Loss: 0.357938, Acc: 0.901438\n",
      "[46/100] Loss: 0.357926, Acc: 0.901806\n",
      "Finish 46 epoch, Loss: 0.356849, Acc: 0.902217\n",
      "Test Loss: 0.339101, Acc: 0.907600\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 47\n",
      "[47/100] Loss: 0.356630, Acc: 0.901458\n",
      "[47/100] Loss: 0.348085, Acc: 0.904583\n",
      "[47/100] Loss: 0.349460, Acc: 0.904583\n",
      "[47/100] Loss: 0.352893, Acc: 0.902995\n",
      "[47/100] Loss: 0.353147, Acc: 0.903562\n",
      "[47/100] Loss: 0.355654, Acc: 0.902604\n",
      "Finish 47 epoch, Loss: 0.355530, Acc: 0.902483\n",
      "Test Loss: 0.337902, Acc: 0.908100\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 48\n",
      "[48/100] Loss: 0.352049, Acc: 0.904063\n",
      "[48/100] Loss: 0.355673, Acc: 0.901510\n",
      "[48/100] Loss: 0.354143, Acc: 0.902569\n",
      "[48/100] Loss: 0.353309, Acc: 0.902578\n",
      "[48/100] Loss: 0.352988, Acc: 0.902729\n",
      "[48/100] Loss: 0.354105, Acc: 0.902674\n",
      "Finish 48 epoch, Loss: 0.354262, Acc: 0.902817\n",
      "Test Loss: 0.336788, Acc: 0.908700\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 49\n",
      "[49/100] Loss: 0.353198, Acc: 0.903125\n",
      "[49/100] Loss: 0.355065, Acc: 0.902656\n",
      "[49/100] Loss: 0.353346, Acc: 0.903472\n",
      "[49/100] Loss: 0.353139, Acc: 0.903125\n",
      "[49/100] Loss: 0.353192, Acc: 0.903083\n",
      "[49/100] Loss: 0.353177, Acc: 0.903142\n",
      "Finish 49 epoch, Loss: 0.353031, Acc: 0.903167\n",
      "Test Loss: 0.335631, Acc: 0.908700\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 50\n",
      "[50/100] Loss: 0.351325, Acc: 0.902604\n",
      "[50/100] Loss: 0.362922, Acc: 0.900365\n",
      "[50/100] Loss: 0.351974, Acc: 0.903958\n",
      "[50/100] Loss: 0.349929, Acc: 0.904609\n",
      "[50/100] Loss: 0.350342, Acc: 0.903813\n",
      "[50/100] Loss: 0.351258, Acc: 0.903333\n",
      "Finish 50 epoch, Loss: 0.351842, Acc: 0.903383\n",
      "Test Loss: 0.334641, Acc: 0.908500\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 51\n",
      "[51/100] Loss: 0.348663, Acc: 0.903021\n",
      "[51/100] Loss: 0.347361, Acc: 0.905000\n",
      "[51/100] Loss: 0.346332, Acc: 0.904688\n",
      "[51/100] Loss: 0.350285, Acc: 0.903490\n",
      "[51/100] Loss: 0.350838, Acc: 0.903729\n",
      "[51/100] Loss: 0.349865, Acc: 0.903733\n",
      "Finish 51 epoch, Loss: 0.350678, Acc: 0.903600\n",
      "Test Loss: 0.333696, Acc: 0.908700\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 52\n",
      "[52/100] Loss: 0.347307, Acc: 0.903542\n",
      "[52/100] Loss: 0.343192, Acc: 0.905156\n",
      "[52/100] Loss: 0.351925, Acc: 0.902639\n",
      "[52/100] Loss: 0.349593, Acc: 0.903880\n",
      "[52/100] Loss: 0.348695, Acc: 0.904417\n",
      "[52/100] Loss: 0.350369, Acc: 0.903611\n",
      "Finish 52 epoch, Loss: 0.349560, Acc: 0.904000\n",
      "Test Loss: 0.332578, Acc: 0.909200\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 53\n",
      "[53/100] Loss: 0.351241, Acc: 0.904896\n",
      "[53/100] Loss: 0.354120, Acc: 0.903698\n",
      "[53/100] Loss: 0.354822, Acc: 0.902569\n",
      "[53/100] Loss: 0.352004, Acc: 0.903125\n",
      "[53/100] Loss: 0.351817, Acc: 0.902625\n",
      "[53/100] Loss: 0.349404, Acc: 0.903715\n",
      "Finish 53 epoch, Loss: 0.348465, Acc: 0.904233\n",
      "Test Loss: 0.331741, Acc: 0.909400\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 54\n",
      "[54/100] Loss: 0.343221, Acc: 0.905312\n",
      "[54/100] Loss: 0.349096, Acc: 0.903229\n",
      "[54/100] Loss: 0.350391, Acc: 0.903542\n",
      "[54/100] Loss: 0.348984, Acc: 0.903880\n",
      "[54/100] Loss: 0.348128, Acc: 0.904208\n",
      "[54/100] Loss: 0.347766, Acc: 0.903906\n",
      "Finish 54 epoch, Loss: 0.347410, Acc: 0.904183\n",
      "Test Loss: 0.330841, Acc: 0.909300\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 55\n",
      "[55/100] Loss: 0.347673, Acc: 0.903021\n",
      "[55/100] Loss: 0.348399, Acc: 0.904219\n",
      "[55/100] Loss: 0.347439, Acc: 0.904479\n",
      "[55/100] Loss: 0.345445, Acc: 0.904818\n",
      "[55/100] Loss: 0.348058, Acc: 0.903958\n",
      "[55/100] Loss: 0.347506, Acc: 0.903941\n",
      "Finish 55 epoch, Loss: 0.346386, Acc: 0.904533\n",
      "Test Loss: 0.329850, Acc: 0.909300\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 56\n",
      "[56/100] Loss: 0.345210, Acc: 0.907708\n",
      "[56/100] Loss: 0.343279, Acc: 0.904740\n",
      "[56/100] Loss: 0.342374, Acc: 0.905000\n",
      "[56/100] Loss: 0.340859, Acc: 0.906042\n",
      "[56/100] Loss: 0.346762, Acc: 0.904646\n",
      "[56/100] Loss: 0.345880, Acc: 0.904774\n",
      "Finish 56 epoch, Loss: 0.345377, Acc: 0.904850\n",
      "Test Loss: 0.328978, Acc: 0.909800\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 57\n",
      "[57/100] Loss: 0.330305, Acc: 0.908333\n",
      "[57/100] Loss: 0.333683, Acc: 0.907969\n",
      "[57/100] Loss: 0.340546, Acc: 0.905833\n",
      "[57/100] Loss: 0.343610, Acc: 0.904818\n",
      "[57/100] Loss: 0.345726, Acc: 0.904312\n",
      "[57/100] Loss: 0.345125, Acc: 0.904688\n",
      "Finish 57 epoch, Loss: 0.344369, Acc: 0.904867\n",
      "Test Loss: 0.328193, Acc: 0.910000\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 58\n",
      "[58/100] Loss: 0.344475, Acc: 0.904271\n",
      "[58/100] Loss: 0.346109, Acc: 0.902865\n",
      "[58/100] Loss: 0.347444, Acc: 0.904167\n",
      "[58/100] Loss: 0.348595, Acc: 0.904115\n",
      "[58/100] Loss: 0.344671, Acc: 0.905292\n",
      "[58/100] Loss: 0.342764, Acc: 0.905312\n",
      "Finish 58 epoch, Loss: 0.343454, Acc: 0.905017\n",
      "Test Loss: 0.327324, Acc: 0.910200\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 59\n",
      "[59/100] Loss: 0.344393, Acc: 0.905104\n",
      "[59/100] Loss: 0.338567, Acc: 0.906771\n",
      "[59/100] Loss: 0.337742, Acc: 0.906319\n",
      "[59/100] Loss: 0.341594, Acc: 0.905937\n",
      "[59/100] Loss: 0.342105, Acc: 0.906125\n",
      "[59/100] Loss: 0.342491, Acc: 0.905608\n",
      "Finish 59 epoch, Loss: 0.342514, Acc: 0.905550\n",
      "Test Loss: 0.326541, Acc: 0.910300\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 60\n",
      "[60/100] Loss: 0.345374, Acc: 0.907812\n",
      "[60/100] Loss: 0.344250, Acc: 0.905885\n",
      "[60/100] Loss: 0.344226, Acc: 0.905243\n",
      "[60/100] Loss: 0.343393, Acc: 0.905417\n",
      "[60/100] Loss: 0.344241, Acc: 0.904979\n",
      "[60/100] Loss: 0.342613, Acc: 0.905295\n",
      "Finish 60 epoch, Loss: 0.341621, Acc: 0.905767\n",
      "Test Loss: 0.325654, Acc: 0.911000\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 61\n",
      "[61/100] Loss: 0.352206, Acc: 0.900625\n",
      "[61/100] Loss: 0.343943, Acc: 0.904948\n",
      "[61/100] Loss: 0.341633, Acc: 0.904792\n",
      "[61/100] Loss: 0.337424, Acc: 0.905911\n",
      "[61/100] Loss: 0.342029, Acc: 0.904938\n",
      "[61/100] Loss: 0.341260, Acc: 0.905278\n",
      "Finish 61 epoch, Loss: 0.340730, Acc: 0.905683\n",
      "Test Loss: 0.324802, Acc: 0.910800\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 62\n",
      "[62/100] Loss: 0.349354, Acc: 0.903125\n",
      "[62/100] Loss: 0.346973, Acc: 0.904063\n",
      "[62/100] Loss: 0.341392, Acc: 0.905937\n",
      "[62/100] Loss: 0.341168, Acc: 0.905234\n",
      "[62/100] Loss: 0.339340, Acc: 0.906187\n",
      "[62/100] Loss: 0.339690, Acc: 0.905833\n",
      "Finish 62 epoch, Loss: 0.339848, Acc: 0.905850\n",
      "Test Loss: 0.324078, Acc: 0.911500\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 63\n",
      "[63/100] Loss: 0.339112, Acc: 0.905312\n",
      "[63/100] Loss: 0.334470, Acc: 0.907344\n",
      "[63/100] Loss: 0.337715, Acc: 0.906007\n",
      "[63/100] Loss: 0.339198, Acc: 0.905312\n",
      "[63/100] Loss: 0.337287, Acc: 0.906187\n",
      "[63/100] Loss: 0.338757, Acc: 0.905937\n",
      "Finish 63 epoch, Loss: 0.339022, Acc: 0.906133\n",
      "Test Loss: 0.323437, Acc: 0.911500\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 64\n",
      "[64/100] Loss: 0.341250, Acc: 0.905833\n",
      "[64/100] Loss: 0.344720, Acc: 0.904948\n",
      "[64/100] Loss: 0.337036, Acc: 0.906701\n",
      "[64/100] Loss: 0.336179, Acc: 0.906224\n",
      "[64/100] Loss: 0.337375, Acc: 0.906396\n",
      "[64/100] Loss: 0.337952, Acc: 0.906354\n",
      "Finish 64 epoch, Loss: 0.338220, Acc: 0.906317\n",
      "Test Loss: 0.322651, Acc: 0.912100\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 65\n",
      "[65/100] Loss: 0.343778, Acc: 0.906458\n",
      "[65/100] Loss: 0.340339, Acc: 0.906198\n",
      "[65/100] Loss: 0.338775, Acc: 0.905486\n",
      "[65/100] Loss: 0.337494, Acc: 0.906536\n",
      "[65/100] Loss: 0.337099, Acc: 0.906771\n",
      "[65/100] Loss: 0.337093, Acc: 0.906719\n",
      "Finish 65 epoch, Loss: 0.337424, Acc: 0.906400\n",
      "Test Loss: 0.321974, Acc: 0.912300\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 66\n",
      "[66/100] Loss: 0.336966, Acc: 0.902396\n",
      "[66/100] Loss: 0.336439, Acc: 0.905260\n",
      "[66/100] Loss: 0.335686, Acc: 0.906319\n",
      "[66/100] Loss: 0.337340, Acc: 0.906068\n",
      "[66/100] Loss: 0.336218, Acc: 0.906438\n",
      "[66/100] Loss: 0.337009, Acc: 0.906476\n",
      "Finish 66 epoch, Loss: 0.336615, Acc: 0.906483\n",
      "Test Loss: 0.321401, Acc: 0.911500\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 67\n",
      "[67/100] Loss: 0.335651, Acc: 0.906771\n",
      "[67/100] Loss: 0.330892, Acc: 0.907656\n",
      "[67/100] Loss: 0.338031, Acc: 0.905625\n",
      "[67/100] Loss: 0.334727, Acc: 0.906563\n",
      "[67/100] Loss: 0.335297, Acc: 0.906146\n",
      "[67/100] Loss: 0.335950, Acc: 0.906510\n",
      "Finish 67 epoch, Loss: 0.335874, Acc: 0.906517\n",
      "Test Loss: 0.320683, Acc: 0.912300\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 68\n",
      "[68/100] Loss: 0.337309, Acc: 0.908125\n",
      "[68/100] Loss: 0.341841, Acc: 0.905312\n",
      "[68/100] Loss: 0.338679, Acc: 0.906632\n",
      "[68/100] Loss: 0.337675, Acc: 0.906432\n",
      "[68/100] Loss: 0.335332, Acc: 0.907375\n",
      "[68/100] Loss: 0.334856, Acc: 0.907014\n",
      "Finish 68 epoch, Loss: 0.335125, Acc: 0.906850\n",
      "Test Loss: 0.320036, Acc: 0.912500\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 69\n",
      "[69/100] Loss: 0.333337, Acc: 0.909063\n",
      "[69/100] Loss: 0.334644, Acc: 0.907083\n",
      "[69/100] Loss: 0.331613, Acc: 0.907917\n",
      "[69/100] Loss: 0.334009, Acc: 0.907500\n",
      "[69/100] Loss: 0.334094, Acc: 0.906937\n",
      "[69/100] Loss: 0.334185, Acc: 0.906910\n",
      "Finish 69 epoch, Loss: 0.334382, Acc: 0.907000\n",
      "Test Loss: 0.319362, Acc: 0.912800\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 70\n",
      "[70/100] Loss: 0.331058, Acc: 0.910000\n",
      "[70/100] Loss: 0.328760, Acc: 0.909219\n",
      "[70/100] Loss: 0.333110, Acc: 0.907951\n",
      "[70/100] Loss: 0.334954, Acc: 0.907422\n",
      "[70/100] Loss: 0.334746, Acc: 0.907250\n",
      "[70/100] Loss: 0.334408, Acc: 0.907222\n",
      "Finish 70 epoch, Loss: 0.333672, Acc: 0.907333\n",
      "Test Loss: 0.318783, Acc: 0.913000\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 71\n",
      "[71/100] Loss: 0.337656, Acc: 0.904479\n",
      "[71/100] Loss: 0.334983, Acc: 0.906198\n",
      "[71/100] Loss: 0.334979, Acc: 0.906910\n",
      "[71/100] Loss: 0.331958, Acc: 0.908698\n",
      "[71/100] Loss: 0.332206, Acc: 0.908146\n",
      "[71/100] Loss: 0.332967, Acc: 0.907969\n",
      "Finish 71 epoch, Loss: 0.332975, Acc: 0.907700\n",
      "Test Loss: 0.318221, Acc: 0.913200\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 72\n",
      "[72/100] Loss: 0.320804, Acc: 0.909688\n",
      "[72/100] Loss: 0.325935, Acc: 0.908646\n",
      "[72/100] Loss: 0.331031, Acc: 0.908160\n",
      "[72/100] Loss: 0.333601, Acc: 0.907500\n",
      "[72/100] Loss: 0.333119, Acc: 0.907354\n",
      "[72/100] Loss: 0.332248, Acc: 0.907760\n",
      "Finish 72 epoch, Loss: 0.332289, Acc: 0.907600\n",
      "Test Loss: 0.317583, Acc: 0.913000\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 73\n",
      "[73/100] Loss: 0.335833, Acc: 0.907188\n",
      "[73/100] Loss: 0.333386, Acc: 0.906771\n",
      "[73/100] Loss: 0.333521, Acc: 0.906319\n",
      "[73/100] Loss: 0.333246, Acc: 0.906901\n",
      "[73/100] Loss: 0.332217, Acc: 0.907792\n",
      "[73/100] Loss: 0.331422, Acc: 0.907882\n",
      "Finish 73 epoch, Loss: 0.331612, Acc: 0.907950\n",
      "Test Loss: 0.317025, Acc: 0.913200\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 74\n",
      "[74/100] Loss: 0.332803, Acc: 0.909167\n",
      "[74/100] Loss: 0.329580, Acc: 0.908594\n",
      "[74/100] Loss: 0.329385, Acc: 0.907882\n",
      "[74/100] Loss: 0.331115, Acc: 0.908125\n",
      "[74/100] Loss: 0.330622, Acc: 0.908896\n",
      "[74/100] Loss: 0.330147, Acc: 0.908559\n",
      "Finish 74 epoch, Loss: 0.330955, Acc: 0.908083\n",
      "Test Loss: 0.316505, Acc: 0.913100\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 75\n",
      "[75/100] Loss: 0.330465, Acc: 0.907188\n",
      "[75/100] Loss: 0.331083, Acc: 0.908854\n",
      "[75/100] Loss: 0.330614, Acc: 0.907708\n",
      "[75/100] Loss: 0.329243, Acc: 0.908568\n",
      "[75/100] Loss: 0.330943, Acc: 0.907917\n",
      "[75/100] Loss: 0.330622, Acc: 0.908368\n",
      "Finish 75 epoch, Loss: 0.330305, Acc: 0.908417\n",
      "Test Loss: 0.315926, Acc: 0.914000\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 76\n",
      "[76/100] Loss: 0.336279, Acc: 0.906563\n",
      "[76/100] Loss: 0.328162, Acc: 0.910000\n",
      "[76/100] Loss: 0.323631, Acc: 0.911146\n",
      "[76/100] Loss: 0.324447, Acc: 0.910365\n",
      "[76/100] Loss: 0.328573, Acc: 0.909229\n",
      "[76/100] Loss: 0.327467, Acc: 0.909271\n",
      "Finish 76 epoch, Loss: 0.329675, Acc: 0.908683\n",
      "Test Loss: 0.315424, Acc: 0.914200\n",
      "Time: 4.9\n",
      "**********\n",
      "epoch 77\n",
      "[77/100] Loss: 0.323152, Acc: 0.911875\n",
      "[77/100] Loss: 0.326379, Acc: 0.909323\n",
      "[77/100] Loss: 0.328991, Acc: 0.908924\n",
      "[77/100] Loss: 0.331477, Acc: 0.908099\n",
      "[77/100] Loss: 0.332223, Acc: 0.907625\n",
      "[77/100] Loss: 0.329346, Acc: 0.908385\n",
      "Finish 77 epoch, Loss: 0.329053, Acc: 0.908750\n",
      "Test Loss: 0.314845, Acc: 0.914500\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 78\n",
      "[78/100] Loss: 0.334000, Acc: 0.908854\n",
      "[78/100] Loss: 0.338260, Acc: 0.906042\n",
      "[78/100] Loss: 0.334600, Acc: 0.907812\n",
      "[78/100] Loss: 0.330472, Acc: 0.908151\n",
      "[78/100] Loss: 0.329232, Acc: 0.908521\n",
      "[78/100] Loss: 0.327739, Acc: 0.909271\n",
      "Finish 78 epoch, Loss: 0.328443, Acc: 0.909150\n",
      "Test Loss: 0.314297, Acc: 0.914700\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 79\n",
      "[79/100] Loss: 0.320246, Acc: 0.912083\n",
      "[79/100] Loss: 0.326221, Acc: 0.911146\n",
      "[79/100] Loss: 0.323419, Acc: 0.911354\n",
      "[79/100] Loss: 0.324207, Acc: 0.910000\n",
      "[79/100] Loss: 0.326647, Acc: 0.909021\n",
      "[79/100] Loss: 0.327954, Acc: 0.909201\n",
      "Finish 79 epoch, Loss: 0.327850, Acc: 0.909200\n",
      "Test Loss: 0.313846, Acc: 0.914300\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 80\n",
      "[80/100] Loss: 0.325620, Acc: 0.908542\n",
      "[80/100] Loss: 0.332087, Acc: 0.906823\n",
      "[80/100] Loss: 0.330927, Acc: 0.907396\n",
      "[80/100] Loss: 0.331654, Acc: 0.908047\n",
      "[80/100] Loss: 0.329562, Acc: 0.908000\n",
      "[80/100] Loss: 0.326165, Acc: 0.909358\n",
      "Finish 80 epoch, Loss: 0.327255, Acc: 0.909317\n",
      "Test Loss: 0.313295, Acc: 0.914800\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 81\n",
      "[81/100] Loss: 0.319101, Acc: 0.910104\n",
      "[81/100] Loss: 0.325396, Acc: 0.909271\n",
      "[81/100] Loss: 0.323839, Acc: 0.909931\n",
      "[81/100] Loss: 0.326151, Acc: 0.909141\n",
      "[81/100] Loss: 0.325075, Acc: 0.909563\n",
      "[81/100] Loss: 0.327260, Acc: 0.909045\n",
      "Finish 81 epoch, Loss: 0.326684, Acc: 0.909417\n",
      "Test Loss: 0.312843, Acc: 0.914800\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 82\n",
      "[82/100] Loss: 0.324094, Acc: 0.910937\n",
      "[82/100] Loss: 0.327589, Acc: 0.908125\n",
      "[82/100] Loss: 0.323886, Acc: 0.909688\n",
      "[82/100] Loss: 0.323746, Acc: 0.910495\n",
      "[82/100] Loss: 0.324485, Acc: 0.910833\n",
      "[82/100] Loss: 0.324451, Acc: 0.910260\n",
      "Finish 82 epoch, Loss: 0.326127, Acc: 0.909733\n",
      "Test Loss: 0.312321, Acc: 0.915200\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 83\n",
      "[83/100] Loss: 0.321535, Acc: 0.911250\n",
      "[83/100] Loss: 0.328183, Acc: 0.910052\n",
      "[83/100] Loss: 0.326339, Acc: 0.911250\n",
      "[83/100] Loss: 0.325198, Acc: 0.910104\n",
      "[83/100] Loss: 0.327128, Acc: 0.909521\n",
      "[83/100] Loss: 0.325825, Acc: 0.909844\n",
      "Finish 83 epoch, Loss: 0.325558, Acc: 0.909683\n",
      "Test Loss: 0.311908, Acc: 0.915100\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 84\n",
      "[84/100] Loss: 0.306795, Acc: 0.915208\n",
      "[84/100] Loss: 0.315721, Acc: 0.912865\n",
      "[84/100] Loss: 0.316444, Acc: 0.912847\n",
      "[84/100] Loss: 0.321786, Acc: 0.911719\n",
      "[84/100] Loss: 0.322653, Acc: 0.911104\n",
      "[84/100] Loss: 0.324486, Acc: 0.910174\n",
      "Finish 84 epoch, Loss: 0.325023, Acc: 0.909917\n",
      "Test Loss: 0.311498, Acc: 0.915300\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 85\n",
      "[85/100] Loss: 0.325855, Acc: 0.909375\n",
      "[85/100] Loss: 0.323677, Acc: 0.909948\n",
      "[85/100] Loss: 0.323253, Acc: 0.910243\n",
      "[85/100] Loss: 0.323724, Acc: 0.911198\n",
      "[85/100] Loss: 0.324312, Acc: 0.910396\n",
      "[85/100] Loss: 0.324174, Acc: 0.910312\n",
      "Finish 85 epoch, Loss: 0.324488, Acc: 0.909917\n",
      "Test Loss: 0.311057, Acc: 0.915200\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 86\n",
      "[86/100] Loss: 0.321885, Acc: 0.909688\n",
      "[86/100] Loss: 0.319639, Acc: 0.910990\n",
      "[86/100] Loss: 0.323590, Acc: 0.909444\n",
      "[86/100] Loss: 0.324367, Acc: 0.909089\n",
      "[86/100] Loss: 0.325480, Acc: 0.909146\n",
      "[86/100] Loss: 0.323718, Acc: 0.909809\n",
      "Finish 86 epoch, Loss: 0.323965, Acc: 0.910050\n",
      "Test Loss: 0.310553, Acc: 0.915300\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 87\n",
      "[87/100] Loss: 0.326802, Acc: 0.906146\n",
      "[87/100] Loss: 0.321461, Acc: 0.908750\n",
      "[87/100] Loss: 0.325308, Acc: 0.907674\n",
      "[87/100] Loss: 0.326731, Acc: 0.908177\n",
      "[87/100] Loss: 0.324894, Acc: 0.909292\n",
      "[87/100] Loss: 0.323429, Acc: 0.910052\n",
      "Finish 87 epoch, Loss: 0.323447, Acc: 0.910333\n",
      "Test Loss: 0.310147, Acc: 0.915400\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 88\n",
      "[88/100] Loss: 0.315427, Acc: 0.912604\n",
      "[88/100] Loss: 0.316584, Acc: 0.912188\n",
      "[88/100] Loss: 0.319458, Acc: 0.911181\n",
      "[88/100] Loss: 0.320882, Acc: 0.911432\n",
      "[88/100] Loss: 0.320259, Acc: 0.910896\n",
      "[88/100] Loss: 0.322449, Acc: 0.910677\n",
      "Finish 88 epoch, Loss: 0.322932, Acc: 0.910500\n",
      "Test Loss: 0.309680, Acc: 0.915400\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 89\n",
      "[89/100] Loss: 0.321914, Acc: 0.909792\n",
      "[89/100] Loss: 0.323050, Acc: 0.909427\n",
      "[89/100] Loss: 0.325414, Acc: 0.909167\n",
      "[89/100] Loss: 0.323357, Acc: 0.909375\n",
      "[89/100] Loss: 0.322073, Acc: 0.910646\n",
      "[89/100] Loss: 0.322131, Acc: 0.910885\n",
      "Finish 89 epoch, Loss: 0.322429, Acc: 0.910550\n",
      "Test Loss: 0.309257, Acc: 0.915400\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 90\n",
      "[90/100] Loss: 0.327184, Acc: 0.908333\n",
      "[90/100] Loss: 0.325175, Acc: 0.909375\n",
      "[90/100] Loss: 0.318550, Acc: 0.910903\n",
      "[90/100] Loss: 0.320949, Acc: 0.910182\n",
      "[90/100] Loss: 0.321523, Acc: 0.910438\n",
      "[90/100] Loss: 0.321532, Acc: 0.910712\n",
      "Finish 90 epoch, Loss: 0.321947, Acc: 0.910583\n",
      "Test Loss: 0.308903, Acc: 0.915500\n",
      "Time: 5.2\n",
      "**********\n",
      "epoch 91\n",
      "[91/100] Loss: 0.308519, Acc: 0.916146\n",
      "[91/100] Loss: 0.313241, Acc: 0.914688\n",
      "[91/100] Loss: 0.318079, Acc: 0.912188\n",
      "[91/100] Loss: 0.321339, Acc: 0.910964\n",
      "[91/100] Loss: 0.320327, Acc: 0.910917\n",
      "[91/100] Loss: 0.321447, Acc: 0.910990\n",
      "Finish 91 epoch, Loss: 0.321449, Acc: 0.910900\n",
      "Test Loss: 0.308463, Acc: 0.915800\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 92\n",
      "[92/100] Loss: 0.328121, Acc: 0.908750\n",
      "[92/100] Loss: 0.327607, Acc: 0.908958\n",
      "[92/100] Loss: 0.321054, Acc: 0.910451\n",
      "[92/100] Loss: 0.319347, Acc: 0.911719\n",
      "[92/100] Loss: 0.320241, Acc: 0.911042\n",
      "[92/100] Loss: 0.320405, Acc: 0.911372\n",
      "Finish 92 epoch, Loss: 0.320984, Acc: 0.911017\n",
      "Test Loss: 0.308059, Acc: 0.916200\n",
      "Time: 5.1\n",
      "**********\n",
      "epoch 93\n",
      "[93/100] Loss: 0.319577, Acc: 0.913646\n",
      "[93/100] Loss: 0.320395, Acc: 0.913177\n",
      "[93/100] Loss: 0.321345, Acc: 0.912014\n",
      "[93/100] Loss: 0.321568, Acc: 0.911432\n",
      "[93/100] Loss: 0.318498, Acc: 0.911521\n",
      "[93/100] Loss: 0.319245, Acc: 0.911354\n",
      "Finish 93 epoch, Loss: 0.320509, Acc: 0.910950\n",
      "Test Loss: 0.307769, Acc: 0.915900\n",
      "Time: 5.0\n",
      "**********\n",
      "epoch 94\n",
      "[94/100] Loss: 0.320363, Acc: 0.912083\n",
      "[94/100] Loss: 0.317149, Acc: 0.914167\n",
      "[94/100] Loss: 0.322618, Acc: 0.911111\n",
      "[94/100] Loss: 0.322019, Acc: 0.910964\n",
      "[94/100] Loss: 0.320533, Acc: 0.911313\n",
      "[94/100] Loss: 0.319947, Acc: 0.911406\n",
      "Finish 94 epoch, Loss: 0.320054, Acc: 0.911417\n",
      "Test Loss: 0.307425, Acc: 0.916300\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 95\n",
      "[95/100] Loss: 0.331545, Acc: 0.907604\n",
      "[95/100] Loss: 0.327905, Acc: 0.909896\n",
      "[95/100] Loss: 0.325267, Acc: 0.909861\n",
      "[95/100] Loss: 0.323196, Acc: 0.910417\n",
      "[95/100] Loss: 0.319296, Acc: 0.911583\n",
      "[95/100] Loss: 0.319468, Acc: 0.911458\n",
      "Finish 95 epoch, Loss: 0.319598, Acc: 0.911383\n",
      "Test Loss: 0.307004, Acc: 0.916400\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 96\n",
      "[96/100] Loss: 0.321319, Acc: 0.911146\n",
      "[96/100] Loss: 0.319671, Acc: 0.910312\n",
      "[96/100] Loss: 0.322547, Acc: 0.910799\n",
      "[96/100] Loss: 0.321035, Acc: 0.910833\n",
      "[96/100] Loss: 0.321619, Acc: 0.910708\n",
      "[96/100] Loss: 0.319294, Acc: 0.911597\n",
      "Finish 96 epoch, Loss: 0.319142, Acc: 0.911617\n",
      "Test Loss: 0.306594, Acc: 0.916300\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 97\n",
      "[97/100] Loss: 0.324660, Acc: 0.911458\n",
      "[97/100] Loss: 0.325155, Acc: 0.910208\n",
      "[97/100] Loss: 0.327512, Acc: 0.908785\n",
      "[97/100] Loss: 0.323374, Acc: 0.910182\n",
      "[97/100] Loss: 0.319934, Acc: 0.911354\n",
      "[97/100] Loss: 0.318708, Acc: 0.911649\n",
      "Finish 97 epoch, Loss: 0.318713, Acc: 0.911500\n",
      "Test Loss: 0.306273, Acc: 0.916700\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 98\n",
      "[98/100] Loss: 0.315544, Acc: 0.910625\n",
      "[98/100] Loss: 0.322966, Acc: 0.909844\n",
      "[98/100] Loss: 0.324737, Acc: 0.909618\n",
      "[98/100] Loss: 0.321849, Acc: 0.910911\n",
      "[98/100] Loss: 0.321611, Acc: 0.911083\n",
      "[98/100] Loss: 0.319774, Acc: 0.911354\n",
      "Finish 98 epoch, Loss: 0.318279, Acc: 0.911767\n",
      "Test Loss: 0.305920, Acc: 0.916500\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 99\n",
      "[99/100] Loss: 0.316599, Acc: 0.913021\n",
      "[99/100] Loss: 0.315379, Acc: 0.914323\n",
      "[99/100] Loss: 0.313069, Acc: 0.914306\n",
      "[99/100] Loss: 0.314977, Acc: 0.913333\n",
      "[99/100] Loss: 0.315167, Acc: 0.913208\n",
      "[99/100] Loss: 0.317848, Acc: 0.912344\n",
      "Finish 99 epoch, Loss: 0.317848, Acc: 0.911950\n",
      "Test Loss: 0.305584, Acc: 0.916700\n",
      "Time: 4.6\n",
      "**********\n",
      "epoch 100\n",
      "[100/100] Loss: 0.319549, Acc: 0.910729\n",
      "[100/100] Loss: 0.314683, Acc: 0.912135\n",
      "[100/100] Loss: 0.315542, Acc: 0.912222\n",
      "[100/100] Loss: 0.315387, Acc: 0.912682\n",
      "[100/100] Loss: 0.317546, Acc: 0.912521\n",
      "[100/100] Loss: 0.317613, Acc: 0.911892\n",
      "Finish 100 epoch, Loss: 0.317430, Acc: 0.912017\n",
      "Test Loss: 0.305208, Acc: 0.916800\n",
      "Time: 4.5\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    print('*' * 10)\n",
    "    print(f'epoch {epoch+1}')\n",
    "    since = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1) # 将图片展开成 28x28\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        # forward propagation\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum().float()\n",
    "        running_acc += num_correct\n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 300 == 0:\n",
    "            print(f'[{epoch+1}/{num_epoches}] Loss: {running_loss/(batch_size*i):.6f}, Acc: {running_acc/(batch_size*i):.6f}')\n",
    "    print(f'Finish {epoch+1} epoch, Loss: {running_loss/(len(train_dataset)):.6f}, Acc: {running_acc/len(train_dataset):.6f}')\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_acc = 0.0\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum().float()\n",
    "        eval_acc += num_correct\n",
    "    print(f'Test Loss: {eval_loss/(len(test_dataset)):.6f}, Acc: {eval_acc/(len(test_dataset)):.6f}')\n",
    "    print(f'Time: {time.time() - since:.1f}')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'save/02-logistic regression.pytorch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
