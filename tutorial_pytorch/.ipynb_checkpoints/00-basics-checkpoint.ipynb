{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "载入基本模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch 基本处理单元\n",
    "\n",
    "返回一个 5x4 的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-29 *\n",
       "       [[ 0.0000,  2.5244,  0.0000,  2.5244],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回一个 5x4 的随机矩阵，随机初始化为 0-1 的均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7773,  0.6592,  0.5887,  0.0207],\n",
      "        [ 0.5215,  0.2389,  0.2157,  0.0721],\n",
      "        [ 0.9549,  0.1544,  0.7500,  0.0738],\n",
      "        [ 0.4069,  0.5866,  0.2458,  0.8653],\n",
      "        [ 0.6489,  0.2139,  0.6559,  0.6520]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取一个矩阵的 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回一个值全为 1 的 5x4 矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 numpy 中数据类型的互相转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46026158 0.9610598  0.07854164 0.89590627]\n",
      " [0.06760716 0.2976725  0.6900923  0.16483873]\n",
      " [0.3443427  0.62159437 0.3240525  0.10489565]\n",
      " [0.1713633  0.3291278  0.63939697 0.15848893]\n",
      " [0.13316464 0.11153817 0.82105887 0.14616597]]\n"
     ]
    }
   ],
   "source": [
    "# convert data type between numpy and torch.Tensor\n",
    "a = torch.rand(5, 4)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  4],\n",
      "        [ 3,  6]])\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3, 4], [3, 6]])\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本运算与 numpy 类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 4)\n",
    "y = torch.rand(5, 4)\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7746,  0.4411,  2.8212,  2.3758],\n",
      "        [ 0.0402,  1.8296,  2.3169,  2.4842],\n",
      "        [ 1.6931,  1.9245,  0.2735,  1.9258],\n",
      "        [ 1.5331,  2.3334,  2.8197,  1.6553],\n",
      "        [ 0.3891,  0.5981,  1.7611,  2.1099]])\n"
     ]
    }
   ],
   "source": [
    "print(c * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1450,  0.1347,  0.8438,  0.0408],\n",
      "        [ 0.0083,  0.0170,  0.1953,  0.0555],\n",
      "        [ 0.0947,  0.0455,  0.0568,  0.4868],\n",
      "        [ 0.4242,  0.0387,  0.0273,  0.4446],\n",
      "        [ 0.1111,  0.1273,  0.3749,  0.4593]])\n"
     ]
    }
   ],
   "source": [
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8198,  1.0635,  1.8377,  0.8435],\n",
      "        [ 0.6359,  0.6378,  1.0251,  0.8951],\n",
      "        [ 0.7321,  0.7125,  0.7147,  1.4002],\n",
      "        [ 1.3411,  0.8276,  0.9690,  1.3575],\n",
      "        [ 0.9867,  0.8379,  1.2257,  1.3563]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8198,  1.0635,  1.8377,  0.8435],\n",
      "        [ 0.6359,  0.6378,  1.0251,  0.8951],\n",
      "        [ 0.7321,  0.7125,  0.7147,  1.4002],\n",
      "        [ 1.3411,  0.8276,  0.9690,  1.3575],\n",
      "        [ 0.9867,  0.8379,  1.2257,  1.3563]])\n"
     ]
    }
   ],
   "source": [
    "print(x.add(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.add_() 函数可以直接改变 x 的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8198,  1.0635,  1.8377,  0.8435],\n",
       "        [ 0.6359,  0.6378,  1.0251,  0.8951],\n",
       "        [ 0.7321,  0.7125,  0.7147,  1.4002],\n",
       "        [ 1.3411,  0.8276,  0.9690,  1.3575],\n",
       "        [ 0.9867,  0.8379,  1.2257,  1.3563]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8198,  1.0635,  1.8377,  0.8435],\n",
      "        [ 0.6359,  0.6378,  1.0251,  0.8951],\n",
      "        [ 0.7321,  0.7125,  0.7147,  1.4002],\n",
      "        [ 1.3411,  0.8276,  0.9690,  1.3575],\n",
      "        [ 0.9867,  0.8379,  1.2257,  1.3563]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：以下复制过程与 numpy 的转换过程也仅为浅层拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At first, a is tensor([ 1.,  1.,  1.]),\n",
      "At first, b is [1. 1. 1.].\n",
      "Now, a is tensor([ 2.,  2.,  2.]),\n",
      "And b is [2. 2. 2.].\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "print(f'At first, a is {a},')\n",
    "b = a.numpy()\n",
    "print(f'At first, b is {b}.')\n",
    "a = a.add_(1)\n",
    "print(f'Now, a is {a},')\n",
    "print(f'And b is {b}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将 torch.Tensor 放到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout whether your machine supports GPU calculation\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4322,  0.3700,  0.1187,  0.9786],\n",
      "        [ 0.8491,  0.7611,  0.9484,  0.1458],\n",
      "        [ 0.4877,  0.3428,  0.4837,  0.2157],\n",
      "        [ 0.9415,  0.2460,  0.9097,  0.0110],\n",
      "        [ 0.4901,  0.0101,  0.8365,  0.1452]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 4)\n",
    "if torch.cuda.is_available():\n",
    "    a = a.cuda()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余可用 cuda 功能的 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9830,  0.7404,  0.6124,  0.9216],\n",
      "        [ 0.6229,  0.4938,  0.2693,  0.7555]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.rand(2, 4)\n",
    "if torch.cuda.is_available():\n",
    "    with torch.cuda.device(1):\n",
    "        print(f'Current GPU device: {torch.cuda.current_device()}')\n",
    "        b = b.cuda()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 nn.DataParallel 替代 multiprocessing\n",
    "\n",
    "大多数涉及批量输入和多个GPU的情况应默认使用`DataParallel`来使用多个GPU。尽管有GIL的存在，单个python进程也可能使多个GPU饱和。\n",
    "\n",
    "从0.1.9版本开始，大量的GPU(8+)可能未被充分利用。然而，这是一个已知的问题，也正在积极开发。和往常一样，测试你的用例吧。\n",
    "\n",
    "调用`multiprocessing`来利用CUDA模型存在重要的注意事项；使用具有多处理功能的CUDA模型有重要的注意事项; 除非就是需要谨慎地满足数据处理需求，否则您的程序很可能会出现错误或未定义的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 的自动求导功能\n",
    "\n",
    "torch 和大部分框架一样有着自动求导功能，\n",
    "\n",
    "> 在0.4.0版本之前，只可以用 torch.autograd.Variable 创建可求导变量，\n",
    "> 旧版本的 Variable 和 Tensor 本质上也没有什么区别，不过 Variable 会放在一个计算图里面，可以进行前向传播和反向传播以及求导\n",
    "> 而在 0.4.0 版本之后，Tensor 和 Variable 合并为一个类\n",
    "\n",
    "![1.png](http://pytorch.org/tutorials/_images/Variable.png)\n",
    "\n",
    "我们可以通过调用 Variable 对象的 **.data** 属性计算得到原始的 Tensor 变量（现在合并后已不推荐使用），而变量的累积梯度可以通过 **.grad** 属性获得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动求导中还有一个重要的功能 function，即每个变量的 **.grad_fn** 属性，该属性记录了创造该变量的函数过程，因此用户起始创造的变量该属性为空。以下即为从 Tensor 中创造变量的示例，其中 requires_grad 表示是否计算梯度，默认值为 False\n",
    "\n",
    "> 旧版本中除 requires_grad，还有 volatite 决定是否求导，现已经抛弃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable x's .grad_fn is None: None\n",
      "Variable z's has .grad_fn: <AddBackward0 object at 0x10eafc5f8>\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "y = Variable(torch.Tensor([5]), requires_grad=True)\n",
    "print(f'Variable x\\'s .grad_fn is None: {x.grad_fn}')\n",
    "z = 2 * x + y + 4\n",
    "print(f'Variable z\\'s has .grad_fn: {z.grad_fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算生成 z 的累积梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终打印关于各变量的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor([ 2.])\n",
      "dz/dy: tensor([ 1.])\n"
     ]
    }
   ],
   "source": [
    "print(f'dz/dx: {x.grad}')\n",
    "print(f'dz/dy: {y.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新版本实现方式：\n",
    "\n",
    "注意 tensor 的初始化中需要加上小数点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x's .grad_fn is None: None\n",
      "Tensor z's has .grad_fn: <AddBackward0 object at 0x10eaf9160>\n",
      "dz/dx: tensor([ 2.])\n",
      "dz/dy: tensor([ 1.])\n"
     ]
    }
   ],
   "source": [
    "x_new = torch.tensor([3.], requires_grad=True)\n",
    "y_new = torch.tensor([5.], requires_grad=True)\n",
    "print(f'Tensor x\\'s .grad_fn is None: {x_new.grad_fn}')\n",
    "z_new = 2 * x_new + y_new + 4\n",
    "print(f'Tensor z\\'s has .grad_fn: {z_new.grad_fn}')\n",
    "z_new.backward()\n",
    "print(f'dz/dx: {x_new.grad}')\n",
    "print(f'dz/dy: {y_new.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络部分\n",
    "\n",
    "所依赖的主要是 torch.nn 和 torch.nn.functional\n",
    "\n",
    "torch.nn 里面有着所有的神经网络的层的操作，其用来构建网络，只有执行一次网络的运算才执行一次；\n",
    "\n",
    "torch.nn.functional 里面包含的接口函数更加复杂，可以实现更灵活的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络的类框架，以下为包含一个卷积层的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # add a convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 2, 3)\n",
    "        self.conv2 = nn.Conv2d(2, 1, 4)\n",
    "        # more information of various layers could be found in pytorch's manual\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward propagation\n",
    "        x = self.conv1(x)\n",
    "        out = self.conv2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化的网络对象可以直接打印其结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0694, -0.3437, -0.1400, -0.5490, -0.7711],\n",
      "          [ 0.0902, -0.0163,  0.0290, -0.1433,  0.1901],\n",
      "          [-0.1620,  0.0847, -0.1452, -0.4983,  0.6122],\n",
      "          [-0.1166,  0.1228, -0.1087, -0.3342, -0.1542],\n",
      "          [-0.4763, -0.1675, -0.7149, -0.0606, -0.2465]]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 10, 10)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
